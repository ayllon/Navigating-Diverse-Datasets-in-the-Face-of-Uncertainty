\glsresetall

\emph{In situ} Data Exploration is an active research area that requires
a multidisciplinary approach: algorithms, data structures, machine learning,
statistics,  data visualization, information sciences, and, unavoidably,
the domain knowledge --- or business understanding ---  provided by an expert.

Going back to the \gls{CRISPDM} model described in  chapter~\ref{chapter:introduction}
and shown in figure~\ref{fig:crispdm}, our initial objective was to identify
gaps in the tooling available for experts to understand data coming as a set of
unprocessed and perhaps inconsistent set of files. These files are not optimized
for access, and any early decision on how to ingest them into a database may be
counterproductive until the dataset is better understood.

The literature survey from chapter~\ref{chapter:literature_review} showed that
solutions for visualization, optimizations, indexing, storage,\ldots abound.
Still, there is little to no mention of assisting users on \emph{understanding}
data schema, especially when it comes to multiple files from diverse origins or
when meta-data is incomplete or inconsistent.
In chapter~\ref{chapter:diverse}, we have seen that users spend a non-negligible
amount of resources just examining the data structure and layout.

The question that followed was: can we leverage the data \emph{distribution} to
assist users in understanding the schema, on seeing how different datasets may come together?
This is particularly important when the data is numerical and uncertain since
one can not just compare tuples individually but needs to inspect distributions.

In the relational world, the \gls{IND} concept comes close to the objective:
parts of a relation contained (included) within another. However, this modeling
relies on the attributes' discrete nature, such as name, date of birth, etc.

In chapter~\ref{chapter:presq} we propose a generalization, the \gls{EDD}, that
relaxes the strict containment relation required by \glspl{IND}.
We then introduce \PresQ, a novel algorithm for \gls{EDD} finding that incorporates
uncertainty into its world modeling, proving that relying on data distribution alone is feasible.
Therefore, \PresQ is applicable in situations where most existing \gls{IND} solutions
are not: when the data is intrinsically uncertain --- measures of physical properties ---,
or when the validation strategy for the inclusion is an approximate heuristic.
The only requirement is that the expectation of false negatives --- i.e., significance
level for statistical tests --- can be estimated.

For the experiments used to evaluate \PresQ, described in ~\ref{sec:presq_experiments},
the statistical test of choice was based on the $k$-nearest neighbors. While this test
performs well for a wide range of datasets, the resulting trained \emph{model} is not
very useful.
Chapter~\ref{chapter:som} introduces a statistical test based on \gls{SOM}, which provides,
in addition to a $p$ value, a trained projection that we can later use for binning and
cross-matching both datasets using the matching set of features. The resulting \gls{SOM}
is also interpretable in case of rejection, which is also helpful in tentative data exploration.

\section{Future work}
\todo{Is it worth to mention dead-ends? i.e., structure on PGN}

\emph{Improving the finding of quasi-cliques in hypergraphs} Via novel algorithms
    or by generalizing some of the many existing techniques~\cite{WU2015693}.

\emph{Data-aware algorithms} For instance, the correlation matrix on both sides of
    the \gls{EDD} is likely to be similar. Perhaps this kind of information can be used to
    augment the algorithms, or inform the traversal.
    
\emph{Domain-knowledge driven} Perhaps via priors, Bayesian two-sample~\cite{soriano2015bayesian}.
\begin{quote}
I propose a multi-resolution method where two-sample comparison is achieved
through carrying out a collection of local two-sample tests, each comparing the cor-
responding pair of probability assignment coefficients on a node in the partition tree
and thus corresponding to the two-sample difference at a particular location and
scale.
\end{quote}
Kind of merges with not-quite EDD, doesn't it? Sounds nice, whole direction, bayesian handling of this stuff.
Enumerate, chain ideas?

\emph{Dimensionality reduction} Searching for quasi-cliques involves exponential
    time complexity on the number of nodes. Thus, applying a dimensionality reduction beforehand
    would reduce the total run-time and also decrease the noise. Nonetheless, a complication
    arises from the premise that we do not know which attributes are shared.

\emph{Not-quite EDD} (i.e. SNR filtering). Some kind of intersection of point of clouds. Convex
    intersection (see notebooks) does not work for dimensions lower than 3 IIRC.
    One can more or less devise an strategy if a single attribute is cut with a simple
    less than or/and greater than (right?) but what if the thing is more complicated,
    like SNR? Correlation between fields become tricky.

\emph{Other type of numerical dependencies} i.e. how to identify files cut among a
    given set of attributes? i.e. coordinate X, Y, tiling. Some kind of continuity test, I guess,
    but how 