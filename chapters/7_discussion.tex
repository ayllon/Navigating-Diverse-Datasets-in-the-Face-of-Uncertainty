
In section~\ref{sec:main_objective}, we defined that the main objective of this thesis
was \emph{to assist users during the exploration of unprocessed,
numerical, raw data distributed across multiple files}.
After a systematic literature mapping,
described in detail in chapter~\ref{chapter:literature_review}, in section~\ref{sec:objectives} we
narrowed the objectives to the \emph{multiple files with unknown, heterogeneous schemas, solely using the intrinsic data distribution}.

With this objective in mind, in chapter~\ref{chapter:diverse}
we proposed the concept of \gls{EDD},
proposed a novel algorithm, \PresQ, to mine these dependencies between multiple
datasets --- chapter~\ref{chapter:presq} ---, and a two-sample, machine-learning statistical
test that can visually assist users in examining how the two samples differ --- 
chapter~\ref{chapter:som}.

In section~\ref{sec:discuss_presq}, we discuss the behavior, performance, and results of
\PresQ, a tool for the discovery of multidimensional \glspl{EDD} via Quasi-Cliques
on Hypergraphs. In section~\ref{sec:discuss_som}, we argue how the
proposed solutions are relevant to the stated objectives. In
section~\ref{sec:threats}, we list the threats to the validity of this work and
what measures we have taken to minimize their impact.

\section{Discovery of Multidimensional Dependencies via Quasi-Cliques on Hypergraphs}
\label{sec:discuss_presq}

Identifying shared attributes between multiple numerical datasets is an interesting
problem. It combines the challenging nature of algorithms devised to find Inclusion Dependencies,
an NP-hard problem~\cite{kantola1992}, with the unavoidable uncertainty of statistical
methods.
This uncertainty reflects as falsely rejected \glspl{EDD} and falsely accepted \glspl{EDD}.

\Find is an algorithm that maps inclusion dependencies to hyper-cliques,
which generally performs  at least as well as the alternatives \cite{Dursch2019}.
It is loosely coupled with the discrete nature of the underlying data.
However, its ability --- and of most, if not all, of the existing algorithms --- to find
high arity \glspl{EDD} will be impaired by the number of false rejections,
which a lower rejection threshold could compensate for. Yet, this
solution increases the number of false detections, which is a known factor that 
significantly degrades its performance, similar to other hypergraph-based methods' \cite{koeller2006heuristic}.
We experimentally confirmed this problem in section \ref{sec:result_quasi_search}.

We propose a new algorithm based on quasi-cliques, where a candidate is accepted
even if some edges are missing. This algorithm has three parameters:

\begin{itemize}
    \item The ratio of missing edges ($\gamma$).
    \item The tolerance on the number of missing edges connecting a node from the quasi-clique ($\Lambda$).
    \item Whether to use the found quasi-clique as seeds.
\end{itemize}

We provide a generalization of this parameterization from regular 2-graphs
\cite{brunato2007effectively} to uniform $n$-graphs in equations~\ref{eq:edge_hyperclique}
and \ref{eq:deg_hyperclique}.

The results showed in the quasi-clique test set (section~\ref{sec:result_quasi_search})
demonstrate that the seed stage of \PresQ provides results close to the original
cliques on uniform $n$-hypergraphs. The growing stage
can recover them even for a high number of missing edges (up to $30\%$) at the expense of a higher run-time.
These results also prove that the degree threshold based on the hypergeometric distribution
offers comparable performance to a hand-picked ratio $\lambda$
while being more stable and predictable.

For real datasets, the ratio of missing edges can be intuitive to configure
(simply $\gamma = 1 - \alpha$, where $\alpha$ is the test significance level), 
but $\lambda$ can be harder to interpret. We propose instead an intuitive and statistically
interpretable method to dynamically adapt the threshold to the degree which is expected
to follow a hypergeometric distribution and can be adjusted based on the quasi-clique
itself, as shown in equation~\ref{eq:deg_hyperclique_hypergeom}.

While our tests on artificial hypergraphs seem to point to the redundancy of the parameter $\Lambda$,
the results shown in the real-world test set (section~\ref{sec:results_real}) prove that for real noisy graphs,
the combination of both performs consistently better than
either of them separately. The $\gamma$ parameter enables recovery from missing edges and,
simultaneously, $\Lambda$ avoids too many false positives due to 
spurious edges. Thanks to them, the efficacy can be kept even while maintaining or 
increasing the significance level of the tests. This reduces the risk of decreased
performance since the density of the graphs can be kept under control.

If a more exhaustive listing of maximal quasi-cliques is required, the initial set of
quasi-cliques can be used as \emph{seeds} to grow other quasi-cliques by adding suitable vertices.
The results shown in  section~\ref{sec:presq_experiments} demonstrate that this method is capable of
finding considerably more maximal quasi-cliques (not contained in any other found quasi-cliques)
at the expense of a higher run-time. This is due to the traversal of the search space and the
validation of the \glspl{EDD} represented by the quasi-cliques.

The loss of accuracy introduced by this growing stage is minor when starting at $n = 3$,
which means that the statistical test could not reject most candidates. However,
most candidates were rejected for an
initial $n = 2$. We consider this is mostly due
to the lack of power of the \gls{kNN} test for low dimensions, which introduces
many spurious edges.

The overall run-time of the \gls{EDD} finding algorithms is heavily influenced by the
chosen parameter values.
A strict parametrization will reject most seed candidates, and the quasi-clique search will fall
into exponential complexity. Conversely, a flexible one will be faster at finding
quasi-clique candidates. Yet, the statistical test will likely reject them, causing their decomposition
into an exponential number of newer, smaller candidates.
Generally, a balanced parametrization based on $\gamma$ and $\Lambda$ is more predictable.

As a final remark, the set of accepted \glspl{EDD} may contain several
false positives depending on the power of the multivariate statistical test.
A second, more detailed pass can refine this original set.
For instance, we can envision a ranking based on the previously discussed \emph{randomness}~\cite{Zhang2010}
to decide which set of \glspl{EDD} is more suitable for cross-matching the datasets.

In conclusion, \PresQ successfully identifies shared sets of attributes
between multiple data files containing raw, numerical data \emph{exclusively} based
on their distribution. This can guide users to cross-match datasets with unknown schemas,
but also to label attributes for which the metadata is lost as long as there is available
one dataset with properly labeled attributes.

Furthermore, \PresQ can also provide insights that drive serendipitous discoveries.
The \gls{AFDS} result shown in figure~\ref{fig:afds} is an example of such a case:
even with a set of files of unknown schema and unknown content, we could infer
a relation between samples confirmed by the paper that published the data.

\section{Two-sample test based on \glsfmtlong{SOM}}
\label{sec:discuss_som}
The results shown in section~\ref{sec:som_results} prove that the non-parametric,
two-sample \gls{SOM} test described in section~\ref{sec:som_chi2} generally outperforms,
in terms of power, other classifier-based two-sample tests, being in some cases
comparable to kernel-based methods. It has the added advantage of generating an
interpretable and usable model: for instance, the resulting \gls{SOM} could be used
as the basis for a \gls{SOM}-\gls{kNN} combined classification model~\cite{silva2011som}.

Like other machine-learning or kernel-based approaches, our method requires some
initial parameters, such as the \gls{SOM} size, to be set by the user. From our
tests, networks of size $O(100)$ neurons generally work well enough, but for more
precise control, the \gls{SOM} size can be set to the lengths of the two largest
principal components~\cite{KOHONEN201352}.

Rosenblatt \etal~\cite{rosenblatt2019better} argue that provable, \emph{proper} test statistics
should be, in general, preferred over heuristic alternatives. However, our
proposal is more oriented toward exploring abundant structured data. In this
case, developing a tailored statistic for all possible combinations is not viable,
and a pragmatic approach is more suitable~\cite{kim2021classification}.

Finally, if the null hypothesis --- that both samples come from the same underlying
distribution --- is rejected, the generated \gls{SOM}  model can be easily visualized
and examined in more detail.
In sections~\ref{subsec:som_oulad} and \ref{subsec:som_eye}, we demonstrated
that we can reject $H_0$ and obtain valuable information after exploring the learned model.
This exercise would not be possible with a black-box method such as
a neural network classifier~\cite{friedman2004multivariate}.

As a final note, \gls{SOM} maps can be generalizable to non-vectorial data
--- i.e., strings --- as long as more than one ordering relation is
defined~\cite{kohonen1982self, KOHONEN201352}.
Therefore, our proposed statistical test could also be used to verify whether
two sets of sequences --- i.e., proteins or DNA --- share their origin.
Unfortunately, since the \gls{SOM} implementation on which we based our implementation
only works with real-valued dimensions~\cite{wittek_somoclu_2017}, we were not
able to test this scenario.

\section{Threats to validity}
\label{sec:threats}

We now identify the internal and external
threats to the validity of our research, and the measures we took to
counteract their effect.

\subsection{Internal validity}

The \PresQ results shown in the experiments described in section~\ref{sec:presq_experiments}
could risk being just a fluctuation,
not due to an underlying algorithmic improvement. However,
the experimental design described in section~\ref{sec:experiment_design} significantly reduces
this possibility, thanks to the randomization of the initial conditions
and the number of measurements. In any event, we made explicit the uncertainties
of our results --- using 95\% confidence intervals or reporting distribution quartiles.

The results summarized in table~\ref{tab:ind2_summary} show that, on average, the
quasi-clique-based searching algorithm consistently performs
better both in terms of run-time and ability to find the maximal \gls{EDD}.
It has enough runs to make the difference significant.
It is worth mentioning that there are proposed heuristics~\cite{koeller2003discovery} to find higher
arity \glspl{EDD}, even when edges are missing, by merging found lower-arity
\glspl{EDD} and testing the resulting \gls{EDD} candidate.
Nonetheless, we consider that the run-time differences are significant enough to make
the quasi-clique-based search a better approach in those cases.
Even so, that heuristic can also be applied to the output of our proposed algorithm.

We implemented \Find and \PresQ from scratch, sharing 
many parts of the code --- i.e., data structures, statistical tests, etc.
While there is room for optimizations, both would benefit.
Since the relative differences would remain similar, we are confident that the gains come from the 
algorithm rather than its implementation.

\medskip

For the proposed two-sample statistical test, the results shown in
section~\ref{sec:som_results} originate from running the tests between 200 and 2000
times, with independent randomized samples.
Again, the comparisons took into account the error in our measurements,
making it easier to assess their significance.

\subsection{External validity}
The \PresQ experiments were run over four different datasets of
diverse nature and from three separate sources. The chosen statistical tests for
uni- and multi-dimensional distributions were not customized to any of them.
However, a better statistical test can be used if the underlying data distribution is more or
less known (or suspected), which may reduce, or even remove, the advantage of the
quasi-clique approach. Although it is also unlikely that the performance would be any worse:
since an entire clique is still a quasi-clique, our algorithm can identify all of them,
similar to the original \Find algorithm.

One significant caveat of our approach is that it may not find
any dependencies if prior filtering has been applied to only one of the two relations
(i.e., signal-to-noise filtering). This is a limitation of the validation
step. This issue was also recognized on the original \Find proposal \cite{koeller2003discovery}.

\medskip

Similarly, the experiments for the two-sample statistical test based on \gls{SOM}
were run over five different datasets and compared with kernel-based and machine-learning-based
alternatives. Given the performance shown by this test, we are confident in
its capabilities.
