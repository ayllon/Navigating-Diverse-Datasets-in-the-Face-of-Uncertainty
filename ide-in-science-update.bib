
@article{ma_omics_2017,
	title = {Omics informatics: {From} scattered individual software tools to integrated workflow management systems},
	volume = {14},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2016.2535251},
	doi = {10.1109/TCBB.2016.2535251},
	abstract = {Omic data analyses pose great informatics challenges. As an emerging subfield of bioinformatics, omics informatics focuses on analyzing multi-omic data efficiently and effectively, and is gaining momentum. There are two underlying trends in the expansion of omics informatics landscape: the explosion of scattered individual omics informatics tools with each of which focuses on a specific task in both single- and multi- omic settings, and the fast-evolving integrated software platforms such as workflow management systems that can assemble multiple tools into pipelines and streamline integrative analysis for complicated tasks. In this survey, we give a holistic view of omics informatics, from scattered individual informatics tools to integrated workflow management systems. We not only outline the landscape and challenges of omics informatics, but also sample a number of widely used and cutting-edge algorithms in omics data analysis to give readers a fine-grained view. We survey various workflow management systems WMSs, classify them into three levels of WMSs from simple software toolkits to integrated multi-omic analytical platforms, and point out the emerging needs for developing intelligent workflow management systems. We also discuss the challenges, strategies and some existing work in systematic evaluation of omics informatics tools. We conclude by providing future perspectives of emerging fields and new frontiers in omics informatics.},
	number = {4},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Ma, Tianle and Zhang, Aidong},
	month = jul,
	year = {2017},
	note = {Number of pages: 21
Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {926--946},
}

@article{orr_probabilistic_2017,
	title = {Probabilistic database summarization for interactive data exploration},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3115404.3115419},
	doi = {10.14778/3115404.3115419},
	abstract = {We present a probabilistic approach to generate a small, query-able summary of a dataset for interactive data exploration. Departing from traditional summarization techniques, we use the Principle of Maximum Entropy to generate a probabilistic representation of the data that can be used to give approximate query answers. We develop the theoretical framework and formulation of our probabilistic representation and show how to use it to answer queries. We then present solving techniques and give three critical optimizations to improve preprocessing time and query accuracy. Lastly, we experimentally evaluate our work using a 5 GB dataset of flights within the United States and a 210 GB dataset from an astronomy particle simulation. While our current work only supports linear queries, we show that our technique can successfully answer queries faster than sampling while introducing, on average, no more error than sampling and can better distinguish between rare and nonexistent values.},
	number = {10},
	journal = {Proc. VLDB Endow.},
	author = {Orr, Laurel and Balazinska, Magdalena and Suciu, Dan},
	month = jun,
	year = {2017},
	note = {Number of pages: 12
Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1154--1165},
}

@article{muller_improved_2018,
	title = {Improved selectivity estimation by combining knowledge from sampling and synopses},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3213880.3213882},
	doi = {10.14778/3213880.3213882},
	abstract = {Estimating selectivities remains a critical task in query processing. Optimizers rely on the accuracy of selectivities when generating execution plans and, in approximate query answering, estimated selectivities affect the quality of the result. Many systems maintain synopses, e.g., histograms, and, in addition, provide sampling facilities. In this paper, we present a novel approach to combine knowledge from synopses and sampling for the purpose of selectivity estimation for conjunctive queries. We first show how to extract information from synopses and sampling such that they are mutually consistent. In a second step, we show how to combine them and decide on an admissible selectivity estimate. We compare our approach to state-of-the-art methods and evaluate the strengths and limitations of each approach.},
	number = {9},
	journal = {Proc. VLDB Endow.},
	author = {Müller, Magnus and Moerkotte, Guido and Kolb, Oliver},
	month = may,
	year = {2018},
	note = {Number of pages: 13
Publisher: VLDB Endowment},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1016--1028},
}

@article{palkar_filter_2018,
	title = {Filter before you parse: {Faster} analytics on raw data with sparser},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3236187.3236207},
	doi = {10.14778/3236187.3236207},
	abstract = {Exploratory big data applications often run on raw unstructured or semi-structured data formats, such as JSON files or text logs. These applications can spend 80–90\% of their execution time parsing the data. In this paper, we propose a new approach for reducing this overhead: apply filters on the data's raw bytestream before parsing. This technique, which we call raw filtering, leverages the features of modern hardware and the high selectivity of queries found in many exploratory applications. With raw filtering, a user-specified query predicate is compiled into a set of filtering primitives called raw filters (RFs). RFs are fast, SIMD-based operators that occasionally yield false positives, but never false negatives. We combine multiple RFs into an RF cascade to decrease the false positive rate and maximize parsing throughput. Because the best RF cascade is data-dependent, we propose an optimizer that dynamically selects the combination of RFs with the best expected throughput, achieving within 10\% of the global optimum cascade while adding less than 1.2\% overhead. We implement these techniques in a system called Sparser, which automatically manages a parsing cascade given a data stream in a supported format (e.g., JSON, Avro, Parquet) and a user query. We show that many real-world applications are highly selective and benefit from Sparser. Across diverse workloads, Sparser accelerates state-of-the-art parsers such as Mison by up to 22 × and improves end-to-end application performance by up to 9 ×.},
	number = {11},
	journal = {Proc. VLDB Endow.},
	author = {Palkar, Shoumik and Abuzaid, Firas and Bailis, Peter and Zaharia, Matei},
	month = jul,
	year = {2018},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {1576--1589},
}

@article{rossi_interactive_2018,
	title = {Interactive visual graph mining and learning},
	volume = {9},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3200764},
	doi = {10.1145/3200764},
	abstract = {This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.},
	number = {5},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Rossi, Ryan A. and Ahmed, Nesreen K. and Zhou, Rong and Eldardiry, Hoda},
	month = jul,
	year = {2018},
	note = {Number of pages: 25
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {RInteractive: Yes, cluster:Visual Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
}

@article{steindorfer_tomany_2018,
	title = {To-many or to-{One}? {All}-in-one! {Efficient} purely functional multi-maps with type-heterogeneous hash-tries},
	volume = {53},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/3296979.3192420},
	doi = {10.1145/3296979.3192420},
	abstract = {An immutable multi-map is a many-to-many map data structure with expected fast insert and lookup operations. This data structure is used for applications processing graphs or many-to-many relations as applied in compilers, runtimes of programming languages, or in static analysis of object-oriented systems. Collection data structures are assumed to carefully balance execution time of operations with memory consumption characteristics and need to scale gracefully from a few elements to multiple gigabytes at least. When processing larger in-memory data sets the overhead of the data structure encoding itself becomes a memory usage bottleneck, dominating the overall performance. In this paper we propose AXIOM, a novel hash-trie data structure that allows for a highly efficient and type-safe multi-map encoding by distinguishing inlined values of singleton sets from nested sets of multi-mappings. AXIOM strictly generalizes over previous hash-trie data structures by supporting the processing of fine-grained type-heterogeneous content on the implementation level (while API and language support for type-heterogeneity are not scope of this paper). We detail the design and optimizations of AXIOM and further compare it against state-of-the-art immutable maps and multi-maps in Java, Scala and Clojure. We isolate key differences using microbenchmarks and validate the resulting conclusions on a case study in static analysis. AXIOM reduces the key-value storage overhead by 1.87x; with specializing and inlining across collection boundaries it improves by 5.1x.},
	number = {4},
	journal = {SIGPLAN Not.},
	author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
	month = jun,
	year = {2018},
	note = {Number of pages: 13
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Data structures, JVM, functional programming, graph, hashtable, many-to-many relation, multi-map, optimization, performance, persistent data structures},
	pages = {283--295},
}

@article{wang_rcindex_2018,
	title = {{RC}-{Index}: {Diversifying} answers to range queries},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3192965.3192969},
	doi = {10.14778/3192965.3192969},
	abstract = {Query result diversification is widely used in data exploration, Web search, and recommendation systems. The problem of returning diversified query results consists of finding a small subset of valid query answers that are representative and different from one another, usually quantified by a diversity score. Most existing techniques for query diversification first compute all valid query results and then find a diverse subset. These techniques are inefficient when the set of valid query results is large. Other work has proposed efficient solutions for restricted application settings, where results are shared across multiple queries. In this paper, our goal is to support result diversification for general range queries over a single relation. We propose the RC-Index, a novel index structure that achieves efficiency by reducing the number of items that must be retrieved by the database to form a diverse set of the desired size (about 1 second for a dataset of 1 million items). Further, we prove that an RC-Index offers strong approximation guarantees. To the best of our knowledge, this is the first index-based diversification method with a guaranteed approximation ratio for range queries.},
	number = {7},
	journal = {Proc. VLDB Endow.},
	author = {Wang, Yue and Meliou, Alexandra and Miklau, Gerome},
	month = mar,
	year = {2018},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {773--786},
}

@article{olma_adaptive_2019,
	title = {Adaptive partitioning and indexing for in situ query processing},
	volume = {29},
	issn = {1066-8888},
	url = {https://doi.org/10.1007/s00778-019-00580-x},
	doi = {10.1007/s00778-019-00580-x},
	abstract = {The constant flux of data and queries alike has been pushing the boundaries of data analysis systems. The increasing size of raw data files has made data loading an expensive operation that delays the data-to-insight time. To alleviate the loading cost, in situ query processing systems operate directly over raw data and offer instant access to data. At the same time, analytical workloads have increasing number of queries. Typically, each query focuses on a constantly shifting—yet small—range. As a result, minimizing the workload latency requires the benefits of indexing in in situ query processing. In this paper, we present an online partitioning and indexing scheme, along with a partitioning and indexing tuner tailored for in situ querying engines. The proposed system design improves query execution time by taking into account user query patterns, to (i) partition raw data files logically and (ii) build lightweight partition-specific indexes for each partition. We build an in situ query engine called Slalom to showcase the impact of our design. Slalom employs adaptive partitioning and builds non-obtrusive indexes in different partitions on-the-fly based on lightweight query access pattern monitoring. As a result of its lightweight nature, Slalom achieves efficient query processing over raw data with minimal memory consumption. Our experimentation with both microbenchmarks and real-life workloads shows that Slalom outperforms state-of-the-art in situ engines and achieves comparable query response times with fully indexed DBMS, offering lower cumulative query execution times for query workloads with increasing size and unpredictable access patterns.},
	number = {1},
	journal = {The VLDB Journal},
	author = {Olma, Matthaios and Karpathiotakis, Manos and Alagiannis, Ioannis and Athanassoulis, Manos and Ailamaki, Anastasia},
	month = nov,
	year = {2019},
	note = {Number of pages: 23
Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {RRaw: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {569--591},
}

@article{qin_making_2019,
	title = {Making data visualization more efficient and effective: {A} survey},
	volume = {29},
	issn = {1066-8888},
	url = {https://doi.org/10.1007/s00778-019-00588-3},
	doi = {10.1007/s00778-019-00588-3},
	abstract = {Data visualization is crucial in today’s data-driven business world, which has been widely used for helping decision making that is closely related to major revenues of many industrial companies. However, due to the high demand of data processing w.r.t. the volume, velocity, and veracity of data, there is an emerging need for database experts to help for efficient and effective data visualization. In response to this demand, this article surveys techniques that make data visualization more efficient and effective. (1) Visualization specifications define how the users can specify their requirements for generating visualizations. (2) Efficient approaches for data visualization process the data and a given visualization specification, which then produce visualizations with the primary target to be efficient and scalable at an interactive speed. (3) Data visualization recommendation is to auto-complete an incomplete specification, or to discover more interesting visualizations based on a reference visualization.},
	number = {1},
	journal = {The VLDB Journal},
	author = {Qin, Xuedi and Luo, Yuyu and Tang, Nan and Li, Guoliang},
	month = nov,
	year = {2019},
	note = {Number of pages: 25
Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Data visualization, Data visualization recommendation, Efficient data visualization, Visualization languages},
	pages = {93--117},
}

@article{rahman_evaluating_2019,
	title = {Evaluating interactive data systems: {Survey} and case studies},
	volume = {29},
	issn = {1066-8888},
	url = {https://doi.org/10.1007/s00778-019-00589-2},
	doi = {10.1007/s00778-019-00589-2},
	abstract = {Interactive query interfaces have become a popular tool for ad hoc data analysis and exploration. Compared with traditional systems that are optimized for throughput or batched performance, these systems focus more on user-centric interactivity. This poses a new class of performance challenges to the backend, which are further exacerbated by the advent of new interaction modes\&nbsp;(e.g., touch, gesture) and query interface paradigms\&nbsp;(e.g., sliders, maps). There is, thus, a need to clearly articulate the evaluation space for interactive systems. In this paper, we extensively survey the literature to guide the development and evaluation of interactive data systems. We highlight unique characteristics of interactive workloads, discuss confounding factors when conducting user studies, and catalog popular metrics for evaluation. We further delineate certain behaviors not captured by these metrics and propose complementary ones to provide a complete picture of interactivity. We demonstrate how to analyze and employ user behavior for system enhancements through three case studies. Our survey and case studies motivate the need for behavior-driven evaluation and optimizations when building interactive interfaces.},
	number = {1},
	journal = {The VLDB Journal},
	author = {Rahman, Protiva and Jiang, Lilong and Nandi, Arnab},
	month = nov,
	year = {2019},
	note = {Number of pages: 28
Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Benchmark, Evaluation, Gestural devices, Human-in-the-loop, Interactive systems, Survey, Touch-screen},
	pages = {119--146},
}

@article{walenz_learning_2019,
	title = {Learning to sample: {Counting} with complex queries},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3368289.3368302},
	doi = {10.14778/3368289.3368302},
	abstract = {We study the problem of efficiently estimating counts for queries involving complex filters, such as user-defined functions, or predicates involving self-joins and correlated subqueries. For such queries, traditional sampling techniques may not be applicable due to the complexity of the filter preventing sampling over joins, and sampling after the join may not be feasible due to the cost of computing the full join. The other natural approach of training and using an inexpensive classifier to estimate the count instead of the expensive predicate suffers from the difficulties in training a good classifier and giving meaningful confidence intervals. In this paper we propose a new method of learning to sample where we combine the best of both worlds by using sampling in two phases. First, we use samples to learn a probabilistic classifier, and then use the classifier to design a stratified sampling method to obtain the final estimates. We theoretically analyze algorithms for obtaining an optimal stratification, and compare our approach with a suite of natural alternatives like quantification learning, weighted and stratified sampling, and other techniques from the literature. We also provide extensive experiments in diverse use cases using multiple real and synthetic datasets to evaluate the quality, efficiency, and robustness of our approach.},
	number = {3},
	journal = {Proc. VLDB Endow.},
	author = {Walenz, Brett and Sintos, Stavros and Roy, Sudeepa and Yang, Jun},
	month = nov,
	year = {2019},
	note = {Number of pages: 13
Publisher: VLDB Endowment},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {390--402},
}

@article{hameed_data_2020,
	title = {Data preparation: {A} survey of commercial tools},
	volume = {49},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/3444831.3444835},
	doi = {10.1145/3444831.3444835},
	abstract = {Raw data are often messy: they follow different encodings, records are not well structured, values do not adhere to patterns, etc. Such data are in general not fit to be ingested by downstream applications, such as data analytics tools, or even by data management systems. The act of obtaining information from raw data relies on some data preparation process. Data preparation is integral to advanced data analysis and data management, not only for data science but for any data-driven applications. Existing data preparation tools are operational and useful, but there is still room for improvement and optimization. With increasing data volume and its messy nature, the demand for prepared data increases day by day.To cater to this demand, companies and researchers are developing techniques and tools for data preparation. To better understand the available data preparation systems, we have conducted a survey to investigate (1) prominent data preparation tools, (2) distinctive tool features, (3) the need for preliminary data processing even for these tools and, (4) features and abilities that are still lacking. We conclude with an argument in support of automatic and intelligent data preparation beyond traditional and simplistic techniques.},
	number = {3},
	journal = {Sigmod Record},
	author = {Hameed, Mazhar and Naumann, Felix},
	month = dec,
	year = {2020},
	note = {Number of pages: 12
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {data cleaning, data quality, data wrangling},
	pages = {18--29},
}

@article{lin_plato_2020,
	title = {Plato: {Approximate} analytics over compressed time series with tight deterministic error guarantees},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3384345.3384357},
	doi = {10.14778/3384345.3384357},
	abstract = {Plato provides fast approximate analytics on time series, by precomputing and storing compressed time series. Plato's key novelty is the delivery of tight deterministic error guarantees for the linear algebra operators over vectors/time series, the inner product operator and arithmetic operators. Composing them allows for evaluating common statistics, such as correlation and cross-correlation. In the offline processing phase, Plato (i) segments each time series into several disjoint segmentations using known fixed-length or variable-length segmentation algorithms; (ii) compresses each segment by a compression function that is coming from a user-chosen compression function family; and (iii) associates to each segment 1 to 3 precomputed error measures. In the online query processing phase, Plato uses the error measures to compute the error guarantees. Importantly, we identify certain compression function families that lead to theoretically and experimentally higher quality guarantees.},
	number = {7},
	journal = {Proc. VLDB Endow.},
	author = {Lin, Chunbin and Boursier, Etienne and Papakonstantinou, Yannis},
	month = mar,
	year = {2020},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1105--1118},
}

@article{trummer_demonstrating_2020,
	title = {Demonstrating the voice-based exploration of large data sets with {CiceroDB}-{Zero}},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3415478.3415496},
	doi = {10.14778/3415478.3415496},
	abstract = {This demonstration enables participants to explore large data sets via voice interfaces. The focus of the demonstration is on methods generating concise speech descriptions of query results, specified by users via voice input. The technical novelty of the demonstrated system lies in the fact that processing overheads are mostly moved into a pre-processing phase, generating speeches for batches of queries defined via templates. Visitors can access the demo via smart speakers on-site or via their own smart phones. They will be able to customize the generated voice descriptions and to tune voice output methods.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Trummer, Immanuel},
	month = sep,
	year = {2020},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {2869--2872},
}

@article{yu_tabula_2020,
	title = {Tabula in action: {A} sampling middleware for interactive geospatial visualization dashboards},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3415478.3415510},
	doi = {10.14778/3415478.3415510},
	abstract = {In this paper, we demonstrate Tabula, a middleware that sits between the data system and the geospatial visualization dashboard to increase user interactivity. The proposed system adopts a sampling cube approach that stores prematerialized spatial samples and allows data scientists to define their own accuracy loss function such that the produced samples can be used for various user-defined visualization tasks. The system ensures that the difference between the sample fed into the dashboard and the raw query answer never exceeds the user-specified loss threshold. For demonstration purposes, we connect Apache Zeppelin, a visualization dashboard, to the system and show how Tabula accelerates interactive visualizations on NYC Taxi Trip data, Yelp review data and San Diego Smart Streetlights data.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Yu, Jia and Chowdhury, Kanchan and Sarwat, Mohamed},
	month = sep,
	year = {2020},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2925--2928},
}

@article{bellavista_decentralised_2021,
	title = {Decentralised learning in federated deployment environments: {A} system-level survey},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3429252},
	doi = {10.1145/3429252},
	abstract = {Decentralised learning is attracting more and more interest because it embodies the principles of data minimisation and focused data collection, while favouring the transparency of purpose specification (i.e., the objective for which a model is built). Cloud-centric-only processing and deep learning are no longer strict necessities to train high-fidelity models; edge devices can actively participate in the decentralised learning process by exchanging meta-level information in place of raw data, thus paving the way for better privacy guarantees. In addition, these new possibilities can relieve the network backbone from unnecessary data transfer and allow it to meet strict low-latency requirements by leveraging on-device model inference. This survey provides a detailed and up-to-date overview of the most recent contributions available in the state-of-the-art decentralised learning literature. In particular, it originally provides the reader audience with a clear presentation of the peculiarities of federated settings, with a novel taxonomy of decentralised learning approaches, and with a detailed description of the most relevant and specific system-level contributions of the surveyed solutions for privacy, communication efficiency, non-IIDness, device heterogeneity, and poisoning defense.},
	number = {1},
	journal = {Acm Computing Surveys},
	author = {Bellavista, Paolo and Foschini, Luca and Mora, Alessio},
	month = feb,
	year = {2021},
	note = {Number of pages: 38
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Decentralised learning, communication efficiency, federated deployment, poisoning defense, privacy},
}

@article{blazquez-garcia_review_2021,
	title = {A review on {Outlier}/{Anomaly} detection in time series data},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3444690},
	doi = {10.1145/3444690},
	abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on unsupervised outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
	number = {3},
	journal = {Acm Computing Surveys},
	author = {Blázquez-García, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
	month = apr,
	year = {2021},
	note = {Number of pages: 33
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Outlier detection, anomaly detection, data mining, software, taxonomy, time series},
}

@article{cai_generative_2021,
	title = {Generative adversarial networks: {A} survey toward private and secure applications},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3459992},
	doi = {10.1145/3459992},
	abstract = {Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model’s compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.},
	number = {6},
	journal = {Acm Computing Surveys},
	author = {Cai, Zhipeng and Xiong, Zuobin and Xu, Honghui and Wang, Peng and Li, Wei and Pan, Yi},
	month = jul,
	year = {2021},
	note = {Number of pages: 38
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Generative adversarial networks, deep learning, privacy and security},
}

@article{celes_mobility_2021,
	title = {Mobility trace analysis for intelligent vehicular networks: {Methods}, models, and applications},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3446679},
	doi = {10.1145/3446679},
	abstract = {Intelligent vehicular networks emerge as a promising technology to provide efficient data communication in transportation systems and smart cities. At the same time, the popularization of devices with attached sensors has allowed the obtaining of a large volume of data with spatiotemporal information from different entities. In this sense, we are faced with a large volume of vehicular mobility traces being recorded. Those traces provide unprecedented opportunities to understand the dynamics of vehicular mobility and provide data-driven solutions. In this article, we give an overview of the main publicly available vehicular mobility traces; then, we present the main issues for preprocessing these traces. Also, we present the methods used to characterize and model mobility data. Finally, we review existing proposals that apply the hidden knowledge extracted from the mobility trace for vehicular networks. This article provides a survey on studies that use vehicular mobility traces and provides a guideline for the proposition of data-driven solutions in the domain of vehicular networks. Moreover, we discuss open research problems and give some directions to undertake them.},
	number = {3},
	journal = {Acm Computing Surveys},
	author = {Celes, Clayson and Boukerche, Azzedine and Loureiro, Antonio A. F.},
	month = apr,
	year = {2021},
	note = {Number of pages: 38
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Vehicular networks, data analysis, data mining, mobility, routing, survey, topology, vanet},
}

@article{gu_serverbased_2021,
	title = {From server-based to client-based machine learning: {A} comprehensive survey},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3424660},
	doi = {10.1145/3424660},
	abstract = {In recent years, mobile devices have gained increasing development with stronger computation capability and larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept of client-based machine learning has been proposed. It leverages the users’ local hardware and local data to solve machine learning sub-problems on mobile devices and only uploads computation results rather than the original data for the optimization of the global model. Such an architecture can not only relieve computation and storage burdens on servers but also protect the users’ sensitive information. Another benefit is the bandwidth reduction because various kinds of local data can be involved in the training process without being uploaded. In this article, we provide a literature review on the progressive development of machine learning from server based to client based. We revisit a number of widely used server-based and client-based machine learning methods and applications. We also extensively discuss the challenges and future directions in this area. We believe that this survey will give a clear overview of client-based machine learning and provide guidelines on applying client-based machine learning to practice.},
	number = {1},
	journal = {Acm Computing Surveys},
	author = {Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu, Chun and Lyu, Chengfei and Wu, Zhihua},
	month = jan,
	year = {2021},
	note = {Number of pages: 36
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Mobile intelligence, decentralized training, distributed system, federated learning, machine learning},
}

@article{peng_fast_2021,
	title = {Fast data series indexing for in-memory data},
	volume = {30},
	issn = {1066-8888},
	url = {https://doi.org/10.1007/s00778-021-00677-2},
	doi = {10.1007/s00778-021-00677-2},
	abstract = {Data series similarity search is a core operation for several data series analysis applications across many different domains. However, the state-of-the-art techniques fail to deliver the time performance required for interactive exploration, or analysis of large data series collections. In this work, we propose MESSI, the first data series index designed for in-memory operation on modern hardware. Our index takes advantage of the modern hardware parallelization opportunities (i.e., SIMD instructions, multi-socket and multi-core architectures), in order to accelerate both index construction and similarity search processing times. Moreover, it benefits from a careful design in the setup and coordination of the parallel workers and data structures, so that it maximizes its performance for in-memory operations. MESSI supports similarity search using both the Euclidean and dynamic time warping (DTW) distances. Our experiments with synthetic and real datasets demonstrate that overall MESSI is up to 4x faster at index construction and up to 11x faster at query answering than the state-of-the-art parallel approach. MESSI is the first to answer exact similarity search queries on 100GB datasets in ∼50\&nbsp;ms (30–75\&nbsp;ms across diverse datasets), which enables real-time, interactive data exploration on very large data series collections.},
	number = {6},
	journal = {The VLDB Journal},
	author = {Peng, Botao and Fatourou, Panagiota and Palpanas, Themis},
	month = jun,
	year = {2021},
	note = {Number of pages: 27
Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {RInteractive: Yes, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1041--1067},
}

@article{wang_are_2021,
	title = {Are we ready for learned cardinality estimation?},
	volume = {14},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3461535.3461552},
	doi = {10.14778/3461535.3461552},
	abstract = {Cardinality estimation is a fundamental but long unresolved problem in query optimization. Recently, multiple papers from different research groups consistently report that learned models have the potential to replace existing cardinality estimators. In this paper, we ask a forward-thinking question: Are we ready to deploy these learned cardinality models in production? Our study consists of three main parts. Firstly, we focus on the static environment (i.e., no data updates) and compare five new learned methods with nine traditional methods on four real-world datasets under a unified workload setting. The results show that learned models are indeed more accurate than traditional methods, but they often suffer from high training and inference costs. Secondly, we explore whether these learned models are ready for dynamic environments (i.e., frequent data updates). We find that they cannot catch up with fast data updates and return large errors for different reasons. For less frequent updates, they can perform better but there is no clear winner among themselves. Thirdly, we take a deeper look into learned models and explore when they may go wrong. Our results show that the performance of learned methods can be greatly affected by the changes in correlation, skewness, or domain size. More importantly, their behaviors are much harder to interpret and often unpredictable. Based on these findings, we identify two promising research directions (control the cost of learned models and make learned models trustworthy) and suggest a number of research opportunities. We hope that our study can guide researchers and practitioners to work together to eventually push learned cardinality estimators into real database systems.},
	number = {9},
	journal = {Proc. VLDB Endow.},
	author = {Wang, Xiaoying and Qu, Changbo and Wu, Weiyuan and Wang, Jiannan and Zhou, Qingqing},
	month = oct,
	year = {2021},
	note = {Number of pages: 15
Publisher: VLDB Endowment},
	keywords = {layer:Database Layer, supercluster:Indexes, type:Validation Research},
	pages = {1640--1654},
}

@article{wang_survey_2021,
	title = {Survey on deep multi-modal data analytics: {Collaboration}, rivalry, and fusion},
	volume = {17},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3408317},
	doi = {10.1145/3408317},
	abstract = {With the development of web technology, multi-modal or multi-view data has surged as a major stream for big data, where each modal/view encodes individual property of data objects. Often, different modalities are complementary to each other. This fact motivated a lot of research attention on fusing the multi-modal feature spaces to comprehensively characterize the data objects. Most of the existing state-of-the-arts focused on how to fuse the energy or information from multi-modal spaces to deliver a superior performance over their counterparts with single modal. Recently, deep neural networks have been exhibited as a powerful architecture to well capture the nonlinear distribution of high-dimensional multimedia data, so naturally does for multi-modal data. Substantial empirical studies are carried out to demonstrate its advantages that are benefited from deep multi-modal methods, which can essentially deepen the fusion from multi-modal deep feature spaces. In this article, we provide a substantial overview of the existing state-of-the-arts in the field of multi-modal data analytics from shallow to deep spaces. Throughout this survey, we further indicate that the critical components for this field go to collaboration, adversarial competition, and fusion over multi-modal spaces. Finally, we share our viewpoints regarding some future directions in this field.},
	number = {1s},
	journal = {ACM Transactions on Multimedia Computing Communications and Applications},
	author = {Wang, Yang},
	month = mar,
	year = {2021},
	note = {Number of pages: 25
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Multi-modal data, deep neural networks},
}

@article{dasilva_fog_2022,
	title = {Fog computing platforms for smart city applications: {A} survey},
	volume = {22},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3488585},
	doi = {10.1145/3488585},
	abstract = {Emerging IoT applications with stringent requirements on latency and data processing have posed many challenges to cloud-centric platforms for Smart Cities. Recently, Fog Computing has been advocated as a promising approach to support such new applications and handle the increasing volume of IoT data and devices. The Fog Computing paradigm is characterized by a horizontal system-level architecture where devices close to end-users and IoT devices are used for processing, storage, and networking functions. Fog Computing platforms aim to facilitate the development of applications and systems for Smart Cities by providing services and abstractions designed to integrate data from IoT devices and various information systems deployed in the city. Despite the potential of the Fog Computing paradigm, the literature still lacks a broad, comprehensive overview of what has been investigated on the use of such paradigm in platforms for Smart Cities and open issues to be addressed in future research and development. In this paper, a systematic mapping study was performed and we present a comprehensive understanding of the use of the Fog Computing paradigm in Smart Cities platforms, providing an overview of the current state of research on this topic, and identifying important gaps in the existing approaches and promising research directions.},
	number = {4},
	journal = {ACM Transactions on Internet Technology},
	author = {Da Silva, Thiago Pereira and Batista, Thais and Lopes, Frederico and Neto, Aluizio Rocha and Delicato, Flávia C. and Pires, Paulo F. and Da Rocha, Atslands R.},
	month = dec,
	year = {2022},
	note = {Number of pages: 32
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Fog computing, edge computing, smart cities},
}

@inproceedings{li_informative_2022,
	address = {New York, NY, USA},
	series = {{MMAsia} '22},
	title = {Informative sample-aware proxy for deep metric learning},
	isbn = {978-1-4503-9478-9},
	url = {https://doi.org/10.1145/3551626.3564942},
	doi = {10.1145/3551626.3564942},
	abstract = {Among various supervised deep metric learning methods proxy-based approaches have achieved high retrieval accuracies. Proxies, which are class-representative points in an embedding space, receive updates based on proxy-sample similarities in a similar manner to sample representations. In existing methods, a relatively small number of samples can produce large gradient magnitudes (i.e., hard samples), and a relatively large number of samples can produce small gradient magnitudes (i.e., easy samples); these can play a major part in updates. Assuming that acquiring too much sensitivity to such extreme sets of samples would deteriorate the generalizability of a method, we propose a novel proxy-based method called Informative Sample-Aware Proxy (Proxy-ISA), which directly modifies a gradient weighting factor for each sample using a scheduled threshold function, so that the model is more sensitive to the informative samples. Extensive experiments on the CUB-200-2011, Cars-196, Stanford Online Products and In-shop Clothes Retrieval datasets demonstrate the superiority of Proxy-ISA compared with the state-of-the-art methods.},
	booktitle = {Proceedings of the 4th {ACM} international conference on multimedia in asia},
	publisher = {Association for Computing Machinery},
	author = {Li, Aoyu and Sato, Ikuro and Ishikawa, Kohta and Kawakami, Rei and Yokota, Rio},
	year = {2022},
	note = {Number of pages: 11
Place: Tokyo, Japan},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
}

@article{personnaz_eda4sum_2022,
	title = {{EDA4SUM}: {Guided} exploration of data summaries},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3554821.3554851},
	doi = {10.14778/3554821.3554851},
	abstract = {We demonstrate EDA4Sum, a framework dedicated to generating guided multi-step data summarization pipelines for very large datasets. Data summarization is the process of producing interpretable and representative subsets of an input dataset. It is usually performed following a one-shot process with the purpose of finding the best summary. EDA4Sum leverages Exploratory Data Analysis (EDA) to produce connected summaries in multiple steps, with the goal of maximizing their cumulative utility. A useful summary contains k individually uniform sets that are collectively diverse to be representative of the input data. EDA4Sum accommodates datasets with different characteristics by providing the ability to tune the weights of uniformity, diversity and novelty when generating multi-step summaries. We demonstrate the superiority of multi-step EDA summarization over single-step summarization for summarizing very large data, and the need to provide guidance to domain experts, by interacting with the VLDB'22 participants who will act as data analysts. The application is avilable at https://bit.ly/eda4sumₐpplication.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Personnaz, Aurélien and Youngmann, Brit and Amer-Yahia, Sihem},
	month = sep,
	year = {2022},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {3590--3593},
}

@article{kraska_northstar_2018,
	title = {Northstar: {An} interactive data science system},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3229863.3240493},
	doi = {10.14778/3229863.3240493},
	abstract = {In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the "guts." Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the "protection" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Kraska, Tim},
	month = aug,
	year = {2018},
	note = {Number of pages: 15
Publisher: VLDB Endowment},
	keywords = {Full Stack, RInteractive: Yes, cluster:Query Approximation, cluster:Visual Tools, layer:Middleware, layer:User Interaction, supercluster:Data Visualization, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2150--2164},
}

@article{kondylakis_coconut_2019,
	title = {Coconut: {A} scalable bottom-up approach for building data series indexes},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3199517.3199519},
	doi = {10.14778/3199517.3199519},
	abstract = {Many modern applications produce massive amounts of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. This leads to two design problems. First, traditional bulk-loading algorithms based on sorting cannot be used. Instead, index construction takes place through slow top-down insertions, which create a non-contiguous index that results in many random I/Os. Second, data series cannot be sorted and split across nodes evenly based on their median value; thus, most leaf nodes are in practice nearly empty. This further slows down query speed and amplifies storage costs. To address these problems, we present Coconut. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading techniques that rely on sorting to quickly build a contiguous index using large sequential disk I/Os. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs.},
	number = {6},
	journal = {Proc. VLDB Endow.},
	author = {Kondylakis, Haridimos and Dayan, Niv and Zoumpatianos, Kostas and Palpanas, Themis},
	month = jan,
	year = {2019},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {677--690},
}

@inproceedings{kamat_unified_2017,
	address = {New York, NY, USA},
	series = {{SSDBM} '17},
	title = {A unified correlation-based approach to sampling over joins},
	isbn = {978-1-4503-5282-6},
	url = {https://doi.org/10.1145/3085504.3085524},
	doi = {10.1145/3085504.3085524},
	abstract = {Supporting sampling in the presence of joins is an important problem in data analysis, but is inherently challenging due to the need to avoid correlation between output tuples. Current solutions provide either correlated or non-correlated samples. Sampling might not always be feasible in the non-correlated sampling-based approaches – the sample size or intermediate data size might be exceedingly large. On the other hand, a correlated sample may not be representative of the join. This paper presents a unified strategy towards join sampling, while considering sample correlation every step of the way. We provide two key contributions. First, in the case where a correlated sample is acceptable, we provide techniques, for all join types, to sample base relations so that their join is as random as possible. Second, in the case where a correlated sample is not acceptable, we provide enhancements to the state-of-the-art algorithms to reduce their execution time and intermediate data size.},
	booktitle = {Proceedings of the 29th international conference on scientific and statistical database management},
	publisher = {Association for Computing Machinery},
	author = {Kamat, Niranjan and Nandi, Arnab},
	year = {2017},
	note = {Number of pages: 12
Place: Chicago, IL, USA},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
}

@inproceedings{kim_optimally_2018,
	address = {New York, NY, USA},
	series = {{HILDA}'18},
	title = {Optimally leveraging density and locality for exploratory browsing and sampling},
	isbn = {978-1-4503-5827-9},
	url = {https://doi.org/10.1145/3209900.3209903},
	doi = {10.1145/3209900.3209903},
	abstract = {Exploratory data analysis often involves repeatedly browsing a small sample of records that satisfy certain predicates. We propose a fast query evaluation engine, called NeedleTail, aimed at letting analysts browse a subset of the query result on large datasets as quickly as possible, independent of the overall size of the result. NeedleTail introduces DensityMaps, a lightweight in-memory indexing structure, and a set of efficient and theoretically sound algorithms to quickly locate promising blocks, trading off locality and density. In settings where the samples are used to compute aggregates, we extend techniques from survey sampling to mitigate the bias in our samples. Our experimental results demonstrate that NeedleTail returns results 7× faster on average on HDDs while occupying up to 23× less memory than existing techniques.},
	booktitle = {Proceedings of the workshop on human-in-the-loop data analytics},
	publisher = {Association for Computing Machinery},
	author = {Kim, Albert and Xu, Liqi and Siddiqui, Tarique and Huang, Silu and Madden, Samuel and Parameswaran, Aditya},
	year = {2018},
	note = {Number of pages: 7
Place: Houston, TX, USA},
	keywords = {RInteractive: Yes, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
}

@article{kang_approximate_2020,
	title = {Approximate selection with guarantees using proxies},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3407790.3407804},
	doi = {10.14778/3407790.3407804},
	abstract = {Due to the falling costs of data acquisition and storage, researchers and industry analysts often want to find all instances of rare events in large datasets. For instance, scientists can cheaply capture thousands of hours of video, but are limited by the need to manually inspect long videos to identify relevant objects and events. To reduce this cost, recent work proposes to use cheap proxy models, such as image classifiers, to identify an approximate set of data points satisfying a data selection filter. Unfortunately, this recent work does not provide the statistical accuracy guarantees necessary in scientific and production settings.In this work, we introduce novel algorithms for approximate selection queries with statistical accuracy guarantees. Namely, given a limited number of exact identifications from an oracle, often a human or an expensive machine learning model, our algorithms meet a minimum precision or recall target with high probability. In contrast, existing approaches can catastrophically fail in satisfying these recall and precision targets. We show that our algorithms can improve query result quality by up to 30 x for both the precision and recall targets in both real and synthetic datasets.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Kang, Daniel and Gan, Edward and Bailis, Peter and Hashimoto, Tatsunori and Zaharia, Matei},
	month = sep,
	year = {2020},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1990--2003},
}

@article{jo_demonstration_2020,
	title = {Demonstration of {ScroogeDB}: {Getting} more bang for the buck with deterministic approximation in the cloud},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3415478.3415519},
	doi = {10.14778/3415478.3415519},
	abstract = {We demonstrate ScroogeDB which aims at minimizing monetary cost of processing aggregation queries in the Cloud. It runs on top of a Cloud database that offers pay-as-you-go query processing where users pay according to the number of bytes processed. ScroogeDB exploits deterministic approximate query processing (DAQ) to achieve monetary savings. That is, ScroogeDB provides deterministic bounds, i.e., bounds that contain the true value with a 100\% probability. ScroogeDB creates small synopses of the database and uses these synopses to answer aggregation queries. By rewriting a query on base tables into a query on smaller synopses, we significantly reduce the amount of processed data. We do not pre-compute synopses in advance of an analysis session. Instead, we generate them on the fly, interleaving synopsis generation with query execution.In our demonstration, we show that our system realizes impressive monetary savings with little precision loss. We run our system on top of the Google BigQuery Cloud platform and provide users with a graphical interface that visualizes deterministic bounds. The graphical interface also provides information regarding the generated synopses and how they contribute to monetary savings.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Jo, Saehan and Pei, Jialing and Trummer, Immanuel},
	month = sep,
	year = {2020},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {UserStudy: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Validation Research},
	pages = {2961--2964},
}

@article{hilprecht_deepdb_2020,
	title = {{DeepDB}: {Learn} from data, not from queries!},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3384345.3384349},
	doi = {10.14778/3384345.3384349},
	abstract = {The typical approach for learned DBMS components is to capture the behavior by running a representative set of queries and use the observations to train a machine learning model. This workload-driven approach, however, has two major downsides. First, collecting the training data can be very expensive, since all queries need to be executed on potentially large databases. Second, training data has to be recollected when the workload or the database changes. To overcome these limitations, we take a different route and propose a new data-driven approach for learned DBMS components which directly supports changes of the workload and data without the need of retraining. Indeed, one may now expect that this comes at a price of lower accuracy since workload-driven approaches can make use of more information. However, this is not the case. The results of our empirical evaluation demonstrate that our data-driven approach not only provides better accuracy than state-ofthe- art learned components but also generalizes better to unseen queries.},
	number = {7},
	journal = {Proc. VLDB Endow.},
	author = {Hilprecht, Benjamin and Schmidt, Andreas and Kulessa, Moritz and Molina, Alejandro and Kersting, Kristian and Binnig, Carsten},
	month = mar,
	year = {2020},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {992--1005},
}

@article{grulich_babelfish_2022,
	title = {Babelfish: {Efficient} execution of polyglot queries},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3489496.3489501},
	doi = {10.14778/3489496.3489501},
	abstract = {Today's users of data processing systems come from different domains, have different levels of expertise, and prefer different programming languages. As a result, analytical workload requirements shifted from relational to polyglot queries involving user-defined functions (UDFs). Although some data processing systems support polyglot queries, they often embed third-party language runtimes. This embedding induces a high performance overhead, as it causes additional data materialization between execution engines.In this paper, we present Babelfish, a novel data processing engine designed for polyglot queries. Babelfish introduces an intermediate representation that unifies queries from different implementation languages. This enables new, holistic optimizations across operator and language boundaries, e.g., operator fusion and workload specialization. As a result, Babelfish avoids data transfers and enables efficient utilization of hardware resources. Our evaluation shows that Babelfish outperforms state-of-the-art data processing systems by up to one order of magnitude and reaches the performance of handwritten code. With Babelfish, we bridge the performance gap between relational and multi-language UDFs and lay the foundation for the efficient execution of future polyglot workloads.},
	number = {2},
	journal = {Proc. VLDB Endow.},
	author = {Grulich, Philipp Marian and Zeuch, Steffen and Markl, Volker},
	month = feb,
	year = {2022},
	note = {Number of pages: 15
Publisher: VLDB Endowment},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {196--210},
}

@article{gan_coopstore_2020,
	title = {{CoopStore}: {Optimizing} precomputed summaries for aggregation},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3407790.3407817},
	doi = {10.14778/3407790.3407817},
	abstract = {An emerging class of data systems partition their data and precompute approximate summaries (i.e., sketches and samples) for each segment to reduce query costs. They can then aggregate and combine the segment summaries to estimate results without scanning the raw data. However, given limited storage space each summary introduces approximation errors that affect query accuracy. For instance, systems that use existing mergeable summaries cannot reduce query error below the error of an individual precomputed summary. We introduce CoopStore, a query system that optimizes item frequency and quantile summaries for accuracy when aggregating over multiple segments. Compared to conventional mergeable summaries, CoopStore leverages additional memory available for summary construction and aggregation to derive a more precise combined result. This reduces error by up to 25 x over interval aggregations and 4.5 x over data cube aggregations on industrial datasets compared to standard summarization methods, with provable worst-case error guarantees.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Gan, Edward and Bailis, Peter and Charikar, Moses},
	month = sep,
	year = {2020},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2174--2187},
}

@article{gan_momentbased_2018,
	title = {Moment-based quantile sketches for efficient high cardinality aggregation queries},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3236187.3236212},
	doi = {10.14778/3236187.3236212},
	abstract = {Interactive analytics increasingly involves querying for quantiles over sub-populations of high cardinality datasets. Data processing engines such as Druid and Spark use mergeable summaries to estimate quantiles, but summary merge times can be a bottleneck during aggregation. We show how a compact and efficiently mergeable quantile sketch can support aggregation workloads. This data structure, which we refer to as the moments sketch, operates with a small memory footprint (200 bytes) and computationally efficient (50ns) merges by tracking only a set of summary statistics, notably the sample moments. We demonstrate how we can efficiently estimate quantiles using the method of moments and the maximum entropy principle, and show how the use of a cascade further improves query time for threshold predicates. Empirical evaluation shows that the moments sketch can achieve less than 1 percent quantile error with 15× less overhead than comparable summaries, improving end query time in the MacroBase engine by up to 7× and the Druid engine by up to 60×.},
	number = {11},
	journal = {Proc. VLDB Endow.},
	author = {Gan, Edward and Ding, Jialin and Tai, Kai Sheng and Sharan, Vatsal and Bailis, Peter},
	month = jul,
	year = {2018},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1647--1660},
}

@article{echihabi_hercules_2022,
	title = {Hercules against data series similarity search},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3547305.3547308},
	doi = {10.14778/3547305.3547308},
	abstract = {We propose Hercules, a parallel tree-based technique for exact similarity search on massive disk-based data series collections. We present novel index construction and query answering algorithms that leverage different summarization techniques, carefully schedule costly operations, optimize memory and disk accesses, and exploit the multi-threading and SIMD capabilities of modern hardware to perform CPU-intensive calculations. We demonstrate the superiority and robustness of Hercules with an extensive experimental evaluation against state-of-the-art techniques, using many synthetic and real datasets, and query workloads of varying difficulty. The results show that Hercules performs up to one order of magnitude faster than the best competitor (which is not always the same). Moreover, Hercules is the only index that outperforms the optimized scan on all scenarios, including the hard query workloads on disk-based datasets.},
	number = {10},
	journal = {Proc. VLDB Endow.},
	author = {Echihabi, Karima and Fatourou, Panagiota and Zoumpatianos, Kostas and Palpanas, Themis and Benbrahim, Houda},
	month = sep,
	year = {2022},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {2005--2018},
}

@article{chandramouli_fishstore_2019,
	title = {{FishStore}: {Fast} ingestion and indexing of raw data},
	volume = {12},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3352063.3352100},
	doi = {10.14778/3352063.3352100},
	abstract = {The last decade has witnessed a huge increase in data being ingested into the cloud from a variety of data sources. The ingested data takes various forms such as JSON, CSV, and binary formats. Traditionally, data is either ingested into storage in raw form, indexed ad-hoc using range indices, or cooked into analytics-friendly columnar formats. None of these solutions is able to handle modern requirements on storage: making the data available immediately for ad-hoc and streaming queries while ingesting at extremely high throughputs. We demonstrate FishStore, our open-source concurrent latch-free storage layer for data with flexible schema. FishStore builds on recent advances in parsing and indexing techniques, and is based on multi-chain hash indexing of dynamically registered predicated subsets of data. We find predicated subset hashing to be a powerful primitive that supports a broad range of queries on ingested data and admits a higher performance (by up to an order of magnitude) implementation than current alternatives.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Chandramouli, Badrish and Xie, Dong and Li, Yinan and Kossmann, Donald},
	month = aug,
	year = {2019},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1922--1925},
}

@article{berg_progressivedb_2019,
	title = {{ProgressiveDB}: {Progressive} data analytics as a middleware},
	volume = {12},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3352063.3352073},
	doi = {10.14778/3352063.3352073},
	abstract = {ProgressiveDB transforms any standard SQL database into a progressive database capable of continuous, approximate query processing. It introduces a few small extensions to the SQL query language that allow clients to express progressive analytical queries. These extensions are processed in the ProgressiveDB middleware that sits between a database application and the underlying database providing interactive query processing as well as query steering capabilities to the user. In our demo, we show how this system allows a database application with a graphical user interface to interact with different backends, while providing the user with immediate feedback during exploratory data exploration of an on-time flight database. ProgressiveDB also supports efficient query steering by providing a new technique, called progressive views, which allows the intermediate results of one progressive query to be shared and reused by multiple concurrent progressive queries with refined scope.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Berg, Lukas and Ziegler, Tobias and Binnig, Carsten and Röhm, Uwe},
	month = aug,
	year = {2019},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1814--1817},
}

@inproceedings{zhao_distributed_2018,
	address = {New York, NY, USA},
	series = {{SSDBM} '18},
	title = {Distributed caching for processing raw arrays},
	isbn = {978-1-4503-6505-5},
	url = {https://doi.org/10.1145/3221269.3221295},
	doi = {10.1145/3221269.3221295},
	abstract = {As applications continue to generate multi-dimensional data at exponentially increasing rates, fast analytics to extract meaningful results is becoming extremely important. The database community has developed array databases that alleviate this problem through a series of techniques. In-situ mechanisms provide direct access to raw data in the original format—without loading and partitioning. Parallel processing scales to the largest datasets. In-memory caching reduces latency when the same data are accessed across a workload of queries. However, we are not aware of any work on distributed caching of multi-dimensional raw arrays. In this paper, we introduce a distributed framework for cost-based caching of multi-dimensional arrays in native format. Given a set of files that contain portions of an array and an online query workload, the framework computes an effective caching plan in two stages. First, the plan identifies the cells to be cached locally from each of the input files by continuously refining an evolving R-tree index. In the second stage, an optimal assignment of cells to nodes that collocates dependent cells in order to minimize the overall data transfer is determined. We design cache eviction and placement heuristic algorithms that consider the historical query workload. A thorough experimental evaluation over two real datasets in three file formats confirms the superiority - by as much as two orders of magnitude - of the proposed framework over existing techniques in terms of cache overhead and workload execution time.},
	booktitle = {Proceedings of the 30th international conference on scientific and statistical database management},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Weijie and Rusu, Florin and Dong, Bin and Wu, Kesheng and Ho, Anna Y. Q. and Nugent, Peter},
	year = {2018},
	note = {Number of pages: 12
Place: Bozen-Bolzano, Italy},
	keywords = {RDistributed: Yes, RRaw: Yes, cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@article{rong_approximate_2020,
	title = {Approximate partition selection for big-data workloads using summary statistics},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3407790.3407848},
	doi = {10.14778/3407790.3407848},
	abstract = {Many big-data clusters store data in large partitions that support access at a coarse, partition-level granularity. As a result, approximate query processing via row-level sampling is inefficient, often requiring reads of many partitions. In this work, we seek to answer queries quickly and approximately by reading a subset of the data partitions and combining partial answers in a weighted manner without modifying the data layout. We illustrate how to efficiently perform this query processing using a set of pre-computed summary statistics, which inform the choice of partitions and weights. We develop novel means of using the statistics to assess the similarity and importance of partitions. Our experiments on several datasets and data layouts demonstrate that to achieve the same relative error compared to uniform partition sampling, our techniques offer from 2.7x to 70x reduction in the number of partitions read, and the statistics stored per partition require fewer than 100KB.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Rong, Kexin and Lu, Yao and Bailis, Peter and Kandula, Srikanth and Levis, Philip},
	month = sep,
	year = {2020},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {RDistributed: Yes, RRaw: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2606--2619},
}

@article{pang_flashview_2017,
	title = {{FlashView}: {An} interactive visual explorer for raw data},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3137765.3137796},
	doi = {10.14778/3137765.3137796},
	abstract = {New data has been generated in an unexpected high speed. To get insight of those data, data analysts will perform a thorough study using state-of-the-art big data analytical tools. Before the analysis starts, a preprocessing is conducted, where data analyst tends to issue a few ad-hoc queries on a new dataset to explore and gain a better understanding. However, it is costly to perform such ad-hoc queries on large scale data using traditional data management systems, e.g., DBMS, because data loading and indexing are very expensive. In this demo, we propose a novel visual data explorer system, FlashView, which omits the loading process by directly querying raw data. FlashView applies approximate query processing technique to achieve real-time query results. It builds both in-memory index and disk index to facilitate the data scanning. It also supports tracking and updating multiple queries concurrently. Note that FlashView is not designed as a replacement of full-fledged DBMS. Instead, it tries to help the analysts quickly understand the characteristics of data, so he/she can selectively load data into the DBMS to do more sophisticated analysis.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Pang, Zhifei and Wu, Sai and Chen, Gang and Chen, Ke and Shou, Lidan},
	month = aug,
	year = {2017},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, RRaw: Yes, cluster:Visual Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1869--1872},
}

@article{meduri_evaluation_2021,
	title = {Evaluation of machine learning algorithms in predicting the next {SQL} query from the future},
	volume = {46},
	issn = {0362-5915},
	url = {https://doi.org/10.1145/3442338},
	doi = {10.1145/3442338},
	abstract = {Prediction of the next SQL query from the user, given her sequence of queries until the current timestep, during an ongoing interaction session of the user with the database, can help in speculative query processing and increased interactivity. While existing machine learning– (ML) based approaches use recommender systems to suggest relevant queries to a user, there has been no exhaustive study on applying temporal predictors to predict the next user issued query.In this work, we experimentally compare ML algorithms in predicting the immediate next future query in an interaction workload, given the current user query or the sequence of queries in a user session thus far. As a part of this, we propose the adaptation of two powerful temporal predictors: (a) Recurrent Neural Networks (RNNs) and (b) a Reinforcement Learning approach called Q-Learning that uses Markov Decision Processes. We represent each query as a comprehensive set of fragment embeddings that not only captures the SQL operators, attributes, and relations but also the arithmetic comparison operators and constants that occur in the query. Our experiments on two real-world datasets show the effectiveness of temporal predictors against the baseline recommender systems in predicting the structural fragments in a query w.r.t. both quality and time. Besides showing that RNNs can be used to synthesize novel queries, we find that exact Q-Learning outperforms RNNs despite predicting the next query entirely from the historical query logs.},
	number = {1},
	journal = {ACM Transactions on Database Systems},
	author = {Meduri, Venkata Vamsikrishna and Chowdhury, Kanchan and Sarwat, Mohamed},
	month = mar,
	year = {2021},
	note = {Number of pages: 46
Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@article{kang_accelerating_2021,
	title = {Accelerating approximate aggregation queries with expensive predicates},
	volume = {14},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3476249.3476285},
	doi = {10.14778/3476249.3476285},
	abstract = {Researchers and industry analysts are increasingly interested in computing aggregation queries over large, unstructured datasets with selective predicates that are computed using expensive deep neural networks (DNNs). As these DNNs are expensive and because many applications can tolerate approximate answers, analysts are interested in accelerating these queries via approximations. Unfortunately, standard approximate query processing techniques to accelerate such queries are not applicable because they assume the result of the predicates are available ahead of time. Furthermore, recent work using cheap approximations (i.e., proxies) do not support aggregation queries with predicates.To accelerate aggregation queries with expensive predicates, we develop and analyze a query processing algorithm that leverages proxies (ABAE). ABAE must account for the key challenge that it may sample records that do not satisfy the predicate. To address this challenge, we first use the proxy to group records into strata so that records satisfying the predicate are ideally grouped into few strata. Given these strata, ABAE uses pilot sampling and plugin estimates to sample according to the optimal allocation. We show that ABAE converges at an optimal rate in a novel analysis of stratified sampling with draws that may not satisfy the predicate. We further show that ABAE outperforms on baselines on six real-world datasets, reducing labeling costs by up to 2.3X.},
	number = {11},
	journal = {Proc. VLDB Endow.},
	author = {Kang, Daniel and Guibas, John and Bailis, Peter and Hashimoto, Tatsunori and Sun, Yi and Zaharia, Matei},
	month = oct,
	year = {2021},
	note = {Number of pages: 14
Publisher: VLDB Endowment},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2341--2354},
}

@article{chamanara_quis_2017,
	title = {{QUIS}: {In}-situ heterogeneous data source querying},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3137765.3137798},
	doi = {10.14778/3137765.3137798},
	abstract = {Existing data integration frameworks are poorly suited for the special requirements of scientists. To answer a specific research question, often, excerpts of data from different sources need to be integrated. The relevant parts and the set of underlying sources may differ from query to query. The analyses also oftentimes involve frequently changing data and exploratory querying. Additionally, The data sources not only store data in different formats, but also provide inconsistent data access functionality. The classic Extract-Transform-Load (ETL) approach seems too complex and time-consuming and does not fit well with interest and expertise of the scientists.With QUIS (QUery In-Situ), we provide a solution for this problem. QUIS is an open source heterogeneous in-situ data querying system. It utilizes a federated query virtualization approach that is built upon plugged-in adapters. QUIS takes a user query and transforms appropriate portions of it into the corresponding computation model on individual data sources and executes it. It complements the segments of the query that the target data sources can not execute. Hence, it guarantees full syntax and semantic support for its language on all data sources. QUIS's in-situ querying facility almost eliminates the time to prepare the data while maintaining a competitive performance and steady scalability.The present demonstration illustrates interesting features of the system: virtual Schemas, heterogeneous joins, and visual query results. We provide a realistic data processing scenario to examine the system's features. Users can interact with QUIS using its desktop workbench, command line interface, or from any R client including RStudio Server.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Chamanara, Javad and König-Ries, Birgitta and Jagadish, H. V.},
	month = aug,
	year = {2017},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {RDistributed: Yes, RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1877--1880},
}

@article{maccioni_crossing_2017,
	title = {Crossing the finish line faster when paddling the data lake with {KAYAK}},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3137765.3137792},
	doi = {10.14778/3137765.3137792},
	abstract = {Paddling in a data lake is strenuous for a data scientist. Being a loosely-structured collection of raw data with little or no meta-information available, the difficulties of extracting insights from a data lake start from the initial phases of data analysis. Indeed, data preparation, which involves many complex operations (such as source and feature selection, exploratory analysis, data profiling, and data curation), is a long and involved activity for navigating the lake before getting precious insights at the finish line.In this framework, we demonstrate KAYAK, a framework that supports data preparation in a data lake with ad-hoc primitives and allows data scientists to cross the finish line sooner. KAYAK takes into account the tolerance of the user in waiting for the primitives' results and it uses incremental execution strategies to produce informative previews of these results. The framework is based on a wise management of metadata and on features that limit human intervention, thus scaling smoothly when the data lake evolves.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Maccioni, Antonio and Torlone, Riccardo},
	month = aug,
	year = {2017},
	note = {Number of pages: 4
Publisher: VLDB Endowment},
	keywords = {RDistributed: Yes, RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1853--1856},
}

@inproceedings{shahrivari_workload_2022,
	title = {Workload prediction for adaptive approximate query processing},
	doi = {10.1109/BigData55660.2022.10020614},
	abstract = {Approximate Query Processing (AQP) enables a trade-off between accuracy and performance to deliver users real-time responses. State-of-the-art AQP relies on forming compact data summaries and approximating the queries using these summaries. By understanding the sequencing of queries in the given workload, engines tune the construction of synopses and buffer them in a warehouse to reduce the cost of subsequent query execution costs. We present Adaptive Approximate Query Processing (AAQP), which predicts future workload and generates the best set of synopses to execute a given query, i.e., those that minimize the predicted workload’s execution time. We equip AAQP with Recurrent Neural Networks (RNN) that are trained with end-user sessions extracted from the historical workload. We conduct several experiments on real-world workloads to show that AAQP can effectively predict the future workload based on the recent queries and adapt the construction of synopses to minimize the workload execution time, almost nearing optimal performance.},
	booktitle = {2022 {IEEE} international conference on big data (big data)},
	author = {Shahrivari, Hamid and Papapetrou, Odysseas and Fletcher, George},
	month = dec,
	year = {2022},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {217--222},
}

@inproceedings{jesenko_visualization_2018,
	address = {New York, NY, USA},
	series = {{ICBDE} '18},
	title = {Visualization and analytics tool for multi-dimensional data},
	isbn = {978-1-4503-6358-7},
	url = {https://doi.org/10.1145/3206157.3206159},
	doi = {10.1145/3206157.3206159},
	abstract = {This paper proposes a novel visualization and analytics tool, which is capable of searching for hidden relationships and patterns within large multi-dimensional data. The goal of the presented tool is to represent the data in novel ways, understandable and useful to the data owner, with new visual and statistical analytics. Various statistics are offered to the user in order to search for linear and nonlinear correlations between multiple variables. Using a simple dataset, we confirmed the suitability of the proposed tool for revealing new relationships and patterns in the used multi-dimensional data.},
	booktitle = {Proceedings of the 2018 international conference on big data and education},
	publisher = {Association for Computing Machinery},
	author = {Jesenko, David and Brumen, Matej and Lukač, Niko and Žalik, Borut and Mongus, Domen},
	year = {2018},
	note = {Number of pages: 5
Place: Honolulu, HI, USA},
	keywords = {cluster:Visual Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {11--15},
}

@inproceedings{burks_vissnippets_2020,
	address = {New York, NY, USA},
	series = {{PEARC} '20},
	title = {{VisSnippets}: {A} web-based system for impromptu collaborative data exploration on large displays},
	isbn = {978-1-4503-6689-2},
	url = {https://doi.org/10.1145/3311790.3396666},
	doi = {10.1145/3311790.3396666},
	abstract = {The VisSnippets system is designed to facilitate effective collaborative data exploration. VisSnippets leverages SAGE2 middleware that enables users to manage the display of digital media content on large displays, thereby providing collaborators with a high-resolution common workspace. Based in JavaScript, VisSnippets provides users with the flexibility to implement and/or select visualization packages and to quickly access data in the cloud. By simplifying the development process, VisSnippets removes the need to scaffold and integrate interactive visualization applications by hand. Users write reusable blocks of code called “snippets” for data retrieval, transformation, and visualization. By composing dataflows from the group’s collective snippet pool, users can quickly execute and explore complementary or contrasting analyses. By giving users the ability to explore alternative scenarios, VisSnippets facilitates parallel work for collaborative data exploration leveraging large-scale displays. We describe the system, its design and implementation, and showcase its flexibility through two example applications.},
	booktitle = {Practice and experience in advanced research computing},
	publisher = {Association for Computing Machinery},
	author = {Burks, Andrew and Renambot, Luc and Johnson, Andrew},
	year = {2020},
	note = {Number of pages: 8
Place: Portland, OR, USA},
	keywords = {cluster:Visual Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {144--151},
}

@inproceedings{poquet_video_2018,
	address = {New York, NY, USA},
	series = {{LAK} '18},
	title = {Video and learning: {A} systematic review (2007–2017)},
	isbn = {978-1-4503-6400-3},
	url = {https://doi.org/10.1145/3170358.3170376},
	doi = {10.1145/3170358.3170376},
	abstract = {Video materials have become an integral part of university learning and teaching practice. While empirical research concerning the use of videos for educational purposes has increased, the literature lacks an overview of the specific effects of videos on diverse learning outcomes. To address such a gap, this paper presents preliminary results of a large-scale systematic review of peer-reviewed empirical studies published from 2007-2017. The study synthesizes the trends observed through the analysis of 178 papers selected from the screening of 2531 abstracts. The findings summarize the effects of manipulating video presentation, content and tasks on learning outcomes, such as recall, transfer, academic achievement, among others. The study points out the gap between large-scale analysis of fine-grained data on video interaction and experimental findings reliant on established psychological instruments. Narrowing this gap is suggested as the future direction for the research on video-based learning.},
	booktitle = {Proceedings of the 8th international conference on learning analytics and knowledge},
	publisher = {Association for Computing Machinery},
	author = {Poquet, Oleksandra and Lim, Lisa and Mirriahi, Negin and Dawson, Shane},
	year = {2018},
	note = {Number of pages: 10
Place: Sydney, New South Wales, Australia},
	keywords = {systematic review, video-based learning},
	pages = {151--160},
}

@inproceedings{tesfatsion_virtualization_2018,
	address = {New York, NY, USA},
	series = {{ICPE} '18},
	title = {Virtualization techniques compared: {Performance}, resource, and power usage overheads in clouds},
	isbn = {978-1-4503-5095-2},
	url = {https://doi.org/10.1145/3184407.3184414},
	doi = {10.1145/3184407.3184414},
	abstract = {Virtualization solutions based on hypervisors or containers are enabling technologies for scalable, flexible, and cost-effective resource sharing. As the fundamental limitations of each technology are yet to be understood, they need to be regularly reevaluated to better understand the trade-off provided by latest technological advances. This paper presents an in-depth quantitative analysis of virtualization overheads in these two groups of systems and their gaps relative to native environments based on a diverse set of workloads that stress CPU, memory, storage, and networking resources. KVM and XEN are used to represent hypervisor-based virtualization, and LXC and Docker for container-based platforms. The systems were evaluated with respect to several cloud resource management dimensions including performance, isolation, resource usage, energy efficiency, start-up time, and density. Our study is useful both to practitioners to understand the current state of the technology in order to make the right decision in the selection, operation and/or design of platforms and to scholars to illustrate how these technologies evolved over time.},
	booktitle = {Proceedings of the 2018 {ACM}/{SPEC} international conference on performance engineering},
	publisher = {Association for Computing Machinery},
	author = {Tesfatsion, Selome Kostentinos and Klein, Cristian and Tordsson, Johan},
	year = {2018},
	note = {Number of pages: 12
Place: Berlin, Germany},
	keywords = {VMS, containers, isolation, performance, power, resource usage, virtualization overhead},
	pages = {145--156},
}

@article{zheng_visualization_2021,
	title = {Visualization of big spatial data using coresets for kernel density estimates},
	volume = {7},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2019.2913655},
	abstract = {The size of large, geo-located datasets has reached scales where visualization of all data points is inefficient. Random sampling is a method to reduce the size of a dataset, yet it can introduce unwanted errors. We describe a method for subsampling of spatial data suitable for creating kernel density estimates from very large data and demonstrate that it results in less error than random sampling. We also introduce a method to ensure that thresholding of low values based on sampled data does not omit any regions above the desired threshold when working with sampled data. We demonstrate the effectiveness of our approach using both, artificial and real-world large geospatial datasets.},
	number = {3},
	journal = {IEEE Transactions on Big Data},
	author = {Zheng, Yan and Ou, Yi and Lex, Alexander and Phillips, Jeff M.},
	month = jul,
	year = {2021},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {524--534},
}

@inproceedings{lin_wfapprox_2020,
	address = {Cham},
	title = {{WFApprox}: {Approximate} {Window} {Functions} {Processing}},
	isbn = {978-3-030-59410-7},
	doi = {10.1007/978-3-030-59410-7_5},
	abstract = {Window functions, despite being supported by all major database systems, are unable to keep up with the steeply growing size of data. Recently, some approximate query process (AQP) systems are proposed to deal with large and complex data in relational databases, which offer us a flexible trade-off between accuracy and efficiency. At the same time, Machine Learning has been adopted extensively to optimize databases due to its powerful ability in dealing with data. However, there have been few publications that consider using AQP techniques especially model-based methods to accelerate window functions processing. This work presents WFApprox, an AQP system based on Machine Learning models aims at efficiently providing an approximate answer for window functions. WFApprox uses Machine Learning models instead of massive data for query answering. Our experimental evaluation shows that WFApprox significantly outperforms the mainstream database systems over TPC-H benchmark.},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	publisher = {Springer International Publishing},
	author = {Lin, Chunbo and Li, Jingdong and Wang, Xiaoling and Lu, Xingjian and Zhang, Ji},
	editor = {Nah, Yunmook and Cui, Bin and Lee, Sang-Won and Yu, Jeffrey Xu and Moon, Yang-Sae and Whang, Steven Euijong},
	year = {2020},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {72--87},
}

@inproceedings{wacharamanotham_transparency_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Transparency of {CHI} research artifacts: {Results} of a self-reported survey},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376448},
	doi = {10.1145/3313831.3376448},
	abstract = {Several fields of science are experiencing a "replication crisis" that has negatively impacted their credibility. Assessing the validity of a contribution via replicability of its experimental evidence and reproducibility of its analyses requires access to relevant study materials, data, and code. Failing to share them limits the ability to scrutinize or build-upon the research, ultimately hindering scientific progress.Understanding how the diverse research artifacts in HCI impact sharing can help produce informed recommendations for individual researchers and policy-makers in HCI. Therefore, we surveyed authors of CHI 2018-2019 papers, asking if they share their papers' research materials and data, how they share them, and why they do not. The results (34\% response rate) show that sharing is uncommon, partly due to misunderstandings about the purpose of sharing and reliable hosting. We conclude with recommendations for fostering open research practices.This paper and all data and materials are freely available at https://osf.io/3bu6t.},
	booktitle = {Proceedings of the 2020 {CHI} conference on human factors in computing systems},
	publisher = {Association for Computing Machinery},
	author = {Wacharamanotham, Chat and Eisenring, Lukas and Haroz, Steve and Echtler, Florian},
	year = {2020},
	note = {Number of pages: 14
Place: Honolulu, HI, USA},
	keywords = {data availability, open data, open science, public data sharing},
	pages = {1--14},
}

@inproceedings{park_verdictdb_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {{VerdictDB}: {Universalizing} approximate query processing},
	isbn = {978-1-4503-4703-7},
	url = {https://doi.org/10.1145/3183713.3196905},
	doi = {10.1145/3183713.3196905},
	abstract = {Despite 25 years of research in academia, approximate query processing (AQP) has had little industrial adoption. One of the major causes of this slow adoption is the reluctance of traditional vendors to make radical changes to their legacy codebases, and the preoccupation of newer vendors (e.g., SQL-on-Hadoop products) with implementing standard features. Additionally, the few AQP engines that are available are each tied to a specific platform and require users to completely abandon their existing databases—an unrealistic expectation given the infancy of the AQP technology. Therefore, we argue that a universal solution is needed: a database-agnostic approximation engine that will widen the reach of this emerging technology across various platforms.Our proposal, called VerdictDB, uses a middleware architecture that requires no changes to the backend database, and thus, can work with all off-the-shelf engines. Operating at the driver-level, VerdictDB intercepts analytical queries issued to the database and rewrites them into another query that, if executed by any standard relational engine, will yield sufficient information for computing an approximate answer. VerdictDB uses the returned result set to compute an approximate answer and error estimates, which are then passed on to the user or application. However, lack of access to the query execution layer introduces significant challenges in terms of generality, correctness, and efficiency. This paper shows how VerdictDB overcomes these challenges and delivers up to 171× speedup (18.45× on average) for a variety of existing engines, such as Impala, Spark SQL, and Amazon Redshift, while incurring less than 2.6\% relative error. VerdictDB is open-sourced under Apache License.},
	booktitle = {Proceedings of the 2018 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Park, Yongjoo and Mozafari, Barzan and Sorenson, Joseph and Wang, Junhao},
	year = {2018},
	note = {Number of pages: 16
Place: Houston, TX, USA},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1461--1476},
}

@article{miranda_topkube_2018,
	title = {{TopKube}: {A} rank-aware data cube for real-time exploration of spatiotemporal data},
	volume = {24},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2017.2671341},
	abstract = {From economics to sports to entertainment and social media, ranking objects according to some notion of importance is a fundamental tool we humans use all the time to better understand our world. With the ever-increasing amount of user-generated content found online, “what's trending” is now a commonplace phrase that tries to capture the zeitgeist of the world by ranking the most popular microblogging hashtags in a given region and time. However, before we can understand what these rankings tell us about the world, we need to be able to more easily create and explore them, given the significant scale of today's data. In this paper, we describe the computational challenges in building a real-time visual exploratory tool for finding top-ranked objects; build on the recent work involving in-memory and rank-aware data cubes to propose TopKube: a data structure that answers top-k queries up to one order of magnitude faster than the previous state of the art; demonstrate the usefulness of our methods using a set of real-world, publicly available datasets; and provide a new set of benchmarks for other researchers to validate their methods and compare to our own.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Miranda, Fabio and Lins, Lauro and Klosowski, James T. and Silva, Claudio T.},
	month = mar,
	year = {2018},
	keywords = {RInteractive: Yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1394--1407},
}

@inproceedings{olma_taster_2019,
	title = {Taster: {Self}-tuning, elastic and online approximate query processing},
	doi = {10.1109/ICDE.2019.00050},
	abstract = {Current Approximate Query Processing (AQP) engines are far from silver-bullet solutions, as they adopt several static design decisions that target specific workloads and deployment scenarios. Offline AQP engines target deployments with large storage budget, and offer substantial performance improvement for predictable workloads, but fail when new query types appear, i.e., due to shifting user interests. To the other extreme, online AQP engines assume that query workloads are unpredictable, and therefore build all samples at query time, without reusing samples (or parts of them) across queries. Clearly, both extremes miss out on different opportunities for optimizing performance and cost. In this paper, we present Taster, a self-tuning, elastic, online AQP engine that synergistically combines the benefits of online and offline AQP. Taster performs online approximation by injecting synopses (samples and sketches) into the query plan, while at the same time it strategically materializes and reuses synopses across queries, and continuously adapts them to changes in the workload and to the available storage resources. Our experimental evaluation shows that Taster adapts to shifting workload and to varying storage budgets, and always matches or significantly outperforms the state-of-the-art performing AQP approaches (online or offline).},
	booktitle = {2019 {IEEE} 35th international conference on data engineering ({ICDE})},
	author = {Olma, Matthaios and Papapetrou, Odysseas and Appuswamy, Raja and Ailamaki, Anastasia},
	month = apr,
	year = {2019},
	note = {ISSN: 2375-026X},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {482--493},
}

@inproceedings{ge_speculative_2019,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {Speculative distributed {CSV} data parsing for big data analytics},
	isbn = {978-1-4503-5643-5},
	url = {https://doi.org/10.1145/3299869.3319898},
	doi = {10.1145/3299869.3319898},
	abstract = {There has been a recent flurry of interest in providing query capability on raw data in today's big data systems. These raw data must be parsed before processing or use in analytics. Thus, a fundamental challenge in distributed big data systems is that of efficient parallel parsing of raw data. The difficulties come from the inherent ambiguity while independently parsing chunks of raw data without knowing the context of these chunks. Specifically, it can be difficult to find the beginnings and ends of fields and records in these chunks of raw data. To parallelize parsing, this paper proposes a speculation-based approach for the CSV format, arguably the most commonly used raw data format. Due to the syntactic and statistical properties of the format, speculative parsing rarely fails and therefore parsing is efficiently parallelized in a distributed setting. Our speculative approach is also robust, meaning that it can reliably detect syntax errors in CSV data. We experimentally evaluate the speculative, distributed parsing approach in Apache Spark using more than 11,000 real-world datasets, and show that our parser produces significant performance benefits over existing methods.},
	booktitle = {Proceedings of the 2019 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Ge, Chang and Li, Yinan and Eilebrecht, Eric and Chandramouli, Badrish and Kossmann, Donald},
	year = {2019},
	note = {Number of pages: 17
Place: Amsterdam, Netherlands},
	keywords = {RRaw: Yes, cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {883--899},
}

@inproceedings{grua_selfadaptation_2019,
	series = {{SEAMS} '19},
	title = {Self-adaptation in mobile apps: {A} systematic literature study},
	url = {https://doi.org/10.1109/SEAMS.2019.00016},
	doi = {10.1109/SEAMS.2019.00016},
	abstract = {With their increase, smartphones have become more integral components of our lives but due to their mobile nature it is not possible to develop a mobile application the same way another software system would be built. In order to always provide the full service, a mobile application needs to be able to detect and deal with changes of context it may be presented with. A suitable method to achieve this goal is self-adaptation. However, as of today it is difficult to have a clear view of existing research on self-adaptation in the context of mobile applications.In this paper, we apply the systematic literature review methodology on selected peer-reviewed papers focusing on self-adaptability in the context of mobile applications. Out of 607 potentially relevant studies, we select 44 primary studies via carefully-defined exclusion and inclusion criteria. We use known modelling dimensions for self-adaptive software systems as our classification framework, which we apply to all selected primary studies. From the synthesized data we obtained, we produce an overview of the state of the art. The results of this study give a solid foundation to plan for future research and practice on engineering self-adaptive mobile applications.},
	booktitle = {Proceedings of the 14th international symposium on software engineering for adaptive and self-managing systems},
	publisher = {IEEE Press},
	author = {Grua, Eoin Martino and Malavolta, Ivano and Lago, Patricia},
	year = {2019},
	note = {Place: Montreal, Quebec, Canada
Number of pages: 12},
	pages = {51--62},
}

@article{liu_smartcube_2020,
	title = {{SmartCube}: {An} adaptive data management architecture for the real-time visualization of spatiotemporal datasets},
	volume = {26},
	doi = {10.1109/TVCG.2019.2934434},
	abstract = {Interactive visualization and exploration of large spatiotemporal data sets is difficult without carefully-designed data preprocessing and management tools. We propose a novel architecture for spatiotemporal data management. The architecture can dynamically update itself based on user queries. Datasets is stored in a tree-like structure to support memory sharing among cuboids in a logical structure of data cubes. An update mechanism is designed to create or remove cuboids on it, according to the analysis of the user queries, with the consideration of memory size limitation. Data structure is dynamically optimized according to different user queries. During a query process, user queries are recorded to predict the performance increment of the new cuboid. The creation or deletion of a cuboid is determined by performance increment. Experiment results show that our prototype system deliveries good performance towards user queries on different spatiotemporal datasets, which costing small memory size with comparable performance compared with other state-of-the-art algorithms.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Can and Wu, Cong and Shao, Hanning and Yuan, Xiaoru},
	year = {2020},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {790--799},
}

@inproceedings{wang_stull_2020,
	title = {{STULL}: {Unbiased} online sampling for visual exploration of large spatiotemporal data},
	doi = {10.1109/VAST50239.2020.00012},
	abstract = {Online sampling-supported visual analytics is increasingly important, as it allows users to explore large datasets with acceptable approximate answers at interactive rates. However, existing online spatiotemporal sampling techniques are often biased, as most researchers have primarily focused on reducing computational latency. Biased sampling approaches select data with unequal probabilities and produce results that do not match the exact data distribution, leading end users to incorrect interpretations. In this paper, we propose a novel approach to perform unbiased online sampling of large spatiotemporal data. The proposed approach ensures the same probability of selection to every point that qualifies the specifications of a user's multidimensional query. To achieve unbiased sampling for accurate representative interactive visualizations, we design a novel data index and an associated sample retrieval plan. Our proposed sampling approach is suitable for a wide variety of visual analytics tasks, e.g., tasks that run aggregate queries of spatiotemporal data. Extensive experiments confirm the superiority of our approach over a state-of-the-art spatial online sampling technique, demonstrating that within the same computational time, data samples generated in our approach are at least 50\% more accurate in representing the actual spatial distribution of the data and enable approximate visualizations to present closer visual appearances to the exact ones.},
	booktitle = {2020 {IEEE} conference on visual analytics science and technology ({VAST})},
	author = {Wang, Guizhen and Guo, Jingjing and Tang, Mingjie and Queiroz Neto, José Florencio de and Yau, Calvin and Daghistani, Anas and Karimzadeh, Morteza and Aref, Walid G. and Ebert, David S.},
	month = oct,
	year = {2020},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {72--83},
}

@inproceedings{lakshminarasimhan_scalable_2018,
	address = {New York, NY, USA},
	series = {{HPDC} '13},
	title = {Scalable in situ scientific data encoding for analytical query processing},
	isbn = {978-1-4503-1910-2},
	url = {https://doi.org/10.1145/2462902.2465527},
	doi = {10.1145/2462902.2465527},
	abstract = {The process of scientific data analysis in high-performance computing environments has been evolving along with the advancement of computing capabilities. With the onset of exascale computing, the increasing gap between compute performance and I/O bandwidth has rendered the traditional method of post-simulation processing a tedious process. Despite the challenges due to increased data production, there exists an opportunity to benefit from "cheap" computing power to perform query-driven exploration and visualization during simulation time. To accelerate such analyses, applications traditionally augment raw data with large indexes, post-simulation, which are then repeatedly utilized for data exploration. However, the generation of current state-of-the-art indexes involve a compute- and memory-intensive processing, thus rendering them inapplicable in an in situ context. In this paper we propose DIRAQ, a parallel in situ, in network data encoding and reorganization technique that enables the transformation of simulation output into a query-efficient form, with negligible runtime overhead to the simulation run. DIRAQ begins with an effective core-local, precision-based encoding approach, which incorporates an embedded compressed index that is 3 – 6x smaller than current state-of-the-art indexing schemes. DIRAQ then applies an in network index merging strategy, enabling the creation of aggregated indexes ideally suited for spatial-context querying that speed up query responses by up to 10x versus alternative techniques. We also employ a novel aggregation strategy that is topology-, data-, and memory-aware, resulting in efficient I/O and yielding overall end-to-end encoding and I/O time that is less than that required to write the raw data with MPI collective I/O.},
	booktitle = {Proceedings of the 22nd international symposium on high-performance parallel and distributed computing},
	publisher = {Association for Computing Machinery},
	author = {Lakshminarasimhan, Sriram and Boyuka, David A. and Pendse, Saurabh V. and Zou, Xiaocheng and Jenkins, John and Vishwanath, Venkatram and Papka, Michael E. and Samatova, Nagiza F.},
	year = {2018},
	note = {Number of pages: 12
Place: New York, New York, USA},
	keywords = {RDistributed: Yes, RRaw: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1--12},
}

@article{maroulis_resourceaware_2023,
	title = {Resource-aware adaptive indexing for in situ visual exploration and analytics},
	volume = {32},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-022-00739-z},
	doi = {10.1007/s00778-022-00739-z},
	abstract = {In in situ data management scenarios, large data files, which do not fit in main memory, must be efficiently handled using commodity hardware, without the overhead of a preprocessing phase or the loading of data into a database. In this work, we study the challenges posed by the visual analysis tasks in in situ scenarios in the presence of memory constraints. We present an indexing scheme and adaptive query evaluation techniques, which enable efficient categorical-based group-by and filter operations, combined with 2D visual interactions, such as exploration of data points on maps or scatter plots. The indexing scheme combines a tile-based structure, which offers efficient visual exploration over the 2D plane, with a tree-based structure, which organizes a tile’s objects based on its categorical values. The index is constructed on-the-fly, resides in main memory, and is built progressively as the user explores parts of the raw file, whereas its structure and level of granularity are adjusted to the user’s exploration areas and type of analysis. To handle the cases where limited resources are available, we introduce a resource-aware index initialization mechanism, we formulate it as an NP-hard optimization problem and we propose two efficient approximation algorithms to solve it. We conduct extensive experiments using real and synthetic datasets and demonstrate that our approach reports interactive query response times (less than 0.04sec) and in most cases is more than 100\$\${\textbackslash}times \$\$faster and performs up to two orders of magnitude less I/O operations compared to existing solutions. The proposed methods are implemented as part of an open-source system for in situ visual exploration and analytics.},
	number = {1},
	journal = {The VLDB Journal},
	author = {Maroulis, Stavros and Bikakis, Nikos and Papastefanatos, George and Vassiliadis, Panos and Vassiliou, Yannis},
	month = jan,
	year = {2023},
	keywords = {RInteractive: Yes, RRaw: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {199--227},
}

@article{mei_rsatree_2020,
	title = {{RSATree}: {Distribution}-aware data representation of large-scale tabular datasets for flexible visual query},
	volume = {26},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2019.2934800},
	abstract = {Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts' flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Mei, Honghui and Chen, Wei and Wei, Yating and Hu, Yuanzhe and Zhou, Shuyue and Lin, Bingru and Zhao, Ying and Xia, Jiazhi},
	month = jan,
	year = {2020},
	keywords = {RInteractive: Yes, cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1161--1171},
}

@inproceedings{biswas_sampling_2022,
	address = {Cham},
	title = {Sampling for scientific data analysis and reduction},
	isbn = {978-3-030-81627-8},
	doi = {10.1007/978-3-030-81627-8_2},
	abstract = {With exascale supercomputers on the horizon, data-driven in situ data reduction is a very important topic that potentially enables post hoc data visualization, reconstruction, and exploration with the goal of minimal information loss. Sophisticated sampling methods provide a fast approximation to the data that can be used as a preview to the simulation output without the need for full data reconstruction. More detailed analysis can then be performed by reconstructing the sampled data set as necessary. Other data reduction methods such as compression techniques can still be used with the sampled outputs to achieve further data reduction. Sampling can be achieved in the spatial domain (which data locations are to be stored?) and/or temporal domain (which time steps to be stored?). Given a spatial location, data-driven sampling approaches take into account its local properties (such as scalar value, local smoothness etc.) and multivariate association among scalar values to determine the importance of a location. For temporal sampling, changes in the local and global properties across time steps are taken into account as importance criteria. In this chapter, spatial sampling approaches are discussed for univariate and multivariate data sets and their use for effective in situ data reduction is demonstrated.},
	booktitle = {In situ visualization for computational science},
	publisher = {Springer International Publishing},
	author = {Biswas, Ayan and Dutta, Soumya and Turton, Terece L. and Ahrens, James},
	editor = {Childs, Hank and Bennett, Janine C. and Garth, Christoph},
	year = {2022},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {11--36},
}

@inproceedings{sundarmurthy_salvaging_2022,
	title = {Salvaging failing and straggling queries},
	doi = {10.1109/ICDE53745.2022.00108},
	abstract = {Interactive time responses are a crucial requirement for users analyzing large amounts of data, typically stored in a relational style data-warehouse where data is partitioned across thousands of nodes for high efficiency and throughput. However, consistently providing quick responses remains a big challenge for two reasons: (1) with data distributed across thousands of nodes, it is highly likely that some nodes are unavailable or are very slow during query execution and, (2) large number of users result in high resource contention which exacerbates the problem of slow and failing nodes. In such situations, systems typically straggle or fail the query resulting in higher latencies and wastage of resources. In this paper, we propose a novel solution to alleviate the failure/straggling problem: use the intermediate results from the partial query execution over available data, and exploit the statistical properties of efficiently partitioned data, particularly, co-hash partitioned data, to provide approximate answers along with confidence bounds. The proposed approach handles aggregate queries that involve joins, group bys, having clauses and a subclass of nested subqueries, covering a large portion of analytical queries. We validate our approach through extensive experiments on the TPC-H dataset and we observe that even with a low data availability of 1\%, our proposed solution provides answers with less than 5\% error.},
	booktitle = {2022 {IEEE} 38th international conference on data engineering ({ICDE})},
	author = {Sundarmurthy, Bruhathi and Deshmukh, Harshad and Koutris, Paris and Naughton, Jeffrey},
	month = may,
	year = {2022},
	note = {ISSN: 2375-026X},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1382--1395},
}

@inproceedings{behm_photon_2022,
	address = {New York, NY, USA},
	series = {{SIGMOD} '22},
	title = {Photon: {A} fast query engine for lakehouse systems},
	isbn = {978-1-4503-9249-5},
	url = {https://doi.org/10.1145/3514221.3526054},
	doi = {10.1145/3514221.3526054},
	abstract = {Many organizations are shifting to a data management paradigm called the "Lakehouse," which implements the functionality of structured data warehouses on top of unstructured data lakes. This presents new challenges for query execution engines. The engine needs to provide good performance on the raw uncurated datasets that are ubiquitous in data lakes, and excellent performance on structured data stored in popular columnar file formats like Apache Parquet. Toward these goals, we present Photon, a vectorized query engine for Lakehouse environments that we developed at Databricks. Photon can outperform existing warehouses on SQL workloads and also supports the Apache Spark API. We discuss the design choices we made in Photon (e.g., vectorization vs. code generation) and describe its integration with our existing SQL and Apache Spark runtimes, its task model, and its memory manager. Photon has accelerated some customer workloads by over 10x and has recently allowed Databricks to set a new audited performance record for the official 100TB TPC-DS benchmark.},
	booktitle = {Proceedings of the 2022 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Behm, Alexander and Palkar, Shoumik and Agarwal, Utkarsh and Armstrong, Timothy and Cashman, David and Dave, Ankur and Greenstein, Todd and Hovsepian, Shant and Johnson, Ryan and Sai Krishnan, Arvind and Leventis, Paul and Luszczak, Ala and Menon, Prashanth and Mokhtar, Mostafa and Pang, Gene and Paranjpye, Sameer and Rahn, Greg and Samwel, Bart and van Bussel, Tom and van Hovell, Herman and Xue, Maryann and Xin, Reynold and Zaharia, Matei},
	year = {2022},
	note = {Number of pages: 14
Place: Philadelphia, PA, USA},
	keywords = {RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {2326--2339},
}

@inproceedings{yang_qdtree_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {Qd-tree: {Learning} data layouts for big data analytics},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3389770},
	doi = {10.1145/3318464.3389770},
	abstract = {Corporations today collect data at an unprecedented and accelerating scale, making the need to run queries on large datasets increasingly important. Technologies such as columnar block-based data organization and compression have become standard practice in most commercial database systems. However, the problem of best assigning records to data blocks on storage is still open. For example, today's systems usually partition data by arrival time into row groups, or range/hash partition the data based on selected fields. For a given workload, however, such techniques are unable to optimize for the important metric of the number of blocks accessed by a query. This metric directly relates to the I/O cost, and therefore performance, of most analytical queries. Further, they are unable to exploit additional available storage to drive this metric down further. In this paper, we propose a new framework called a query-data routing tree, or qd-tree, to address this problem, and propose two algorithms for their construction based on greedy and deep reinforcement learning techniques. Experiments over benchmark and real workloads show that a qd-tree can provide physical speedups of more than an order of magnitude compared to current blocking schemes, and can reach within 2X of the lower bound for data skipping based on selectivity, while providing complete semantic descriptions of created blocks.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Yang, Zongheng and Chandramouli, Badrish and Wang, Chi and Gehrke, Johannes and Li, Yinan and Minhas, Umar Farooq and Larson, Per-Åke and Kossmann, Donald and Acharya, Rajeev},
	year = {2020},
	note = {Number of pages: 16
Place: Portland, OR, USA},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {193--208},
}

@inproceedings{leventidis_queryvis_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {{QueryVis}: {Logic}-based diagrams help users understand complicated {SQL} queries faster},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3389767},
	doi = {10.1145/3318464.3389767},
	abstract = {Understanding the meaning of existing SQL queries is critical for code maintenance and reuse. Yet SQL can be hard to read, even for expert users or the original creator of a query. We conjecture that it is possible to capture the logical intent of queries in automatically-generated visual diagrams that can help users understand the meaning of queries faster and more accurately than SQL text alone. We present initial steps in that direction with visual diagrams that are based on the first-order logic foundation of SQL and can capture the meaning of deeply nested queries. Our diagrams build upon a rich history of diagrammatic reasoning systems in logic and were designed using a large body of human-computer interaction best practices: they are minimal in that no visual element is superfluous; they are unambiguous in that no two queries with different semantics map to the same visualization; and they extend previously existing visual representations of relational schemata and conjunctive queries in a natural way. An experimental evaluation involving 42 users on Amazon Mechanical Turk shows that with only a 2–3 minute static tutorial, participants could interpret queries meaningfully faster with our diagrams than when reading SQL alone. Moreover, we have evidence that our visual diagrams result in participants making fewer errors than with SQL. We believe that more regular exposure to diagrammatic representations of SQL can give rise to a pattern-based and thus more intuitive use and re-use of SQL. A full version of this paper with all appendices and supplemental material for the experimental study (stimuli, raw data, and analysis code) are available at https://osf.io/btszh.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Leventidis, Aristotelis and Zhang, Jiahui and Dunne, Cody and Gatterbauer, Wolfgang and Jagadish, H.V. and Riedewald, Mirek},
	year = {2020},
	note = {Number of pages: 16
Place: Portland, OR, USA},
	keywords = {UserStudy: Yes, cluster:Visual Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {2303--2318},
}

@inproceedings{patel_query_2019,
	address = {Singapore},
	title = {Query {Morphing}: {A} {Proximity}-{Based} {Data} {Exploration} for {Query} {Reformulation}},
	isbn = {978-981-13-1132-1},
	doi = {10.1007/978-981-13-1132-1_20},
	abstract = {With the increase of information technology, multiple terabytes of structured and unstructured data are generated on daily basis through various sources, such as sensors, lab simulations, social media, web blogs, etc. Due to big data occurrences, acquisition of relevant information is getting complex processing task. These data are often stored and kept in the vast schema, and thus formulating data retrieval requires a fundamental understanding of the schema and content. A discovery-oriented search mechanism delivers good results here, as the user can stepwise explore the database and stop when the result content and quality meet. In this, a naïve user often transforms data request in order to discover relevant items; morphing is a historical approach for the generation of various transformations of input. We proposed “Query Morphing”, an approach for query reformulation based on data exploration. Various design issues and implementation constraints of the proposed approach are also listed.},
	booktitle = {Computational {Intelligence}: {Theories}, {Applications} and {Future} {Directions} - {Volume} {I}},
	publisher = {Springer Singapore},
	author = {Patel, Jay and Singh, Vikram},
	editor = {Verma, Nishchal K. and Ghosh, A. K.},
	year = {2019},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {247--259},
}

@inproceedings{gao_navigating_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {Navigating the data lake with {DATAMARAN}: {Automatically} extracting structure from log datasets},
	isbn = {978-1-4503-4703-7},
	url = {https://doi.org/10.1145/3183713.3183746},
	doi = {10.1145/3183713.3183746},
	abstract = {Organizations routinely accumulate semi-structured log datasets generated as the output of code; these datasets remain unused and uninterpreted, and occupy wasted space—this phenomenon has been colloquially referred to as "data lake” problem. One approach to leverage these semi-structured datasets is to convert them into a structured relational format, following which they can be analyzed in conjunction with other datasets. We present DATAMARAN, an tool that extracts structure from semi-structured log datasets with no human supervision. DATAMARAN automatically identifies field and record endpoints, separates the structured parts from the unstructured noise or formatting, and can tease apart multiple structures from within a dataset, in order to efficiently extract structured relational datasets from semi-structured log datasets, at scale with high accuracy. Compared to other unsupervised log dataset extraction tools developed in prior work, DATAMARAN does not require the record boundaries to be known beforehand, making it much more applicable to the noisy log files that are ubiquitous in data lakes. DATAMARAN can successfully extract structured information from all datasets used in prior work, and can achieve 95\% extraction accuracy on automatically collected log datasets from GitHub—a substantial 66\% increase of accuracy compared to unsupervised schemes from prior work. Our user study further demonstrates that the extraction results of DATAMARAN are closer to the desired structure than competing algorithms.},
	booktitle = {Proceedings of the 2018 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Gao, Yihan and Huang, Silu and Parameswaran, Aditya},
	year = {2018},
	note = {Number of pages: 16
Place: Houston, TX, USA},
	keywords = {RRaw: Yes, UserStudy: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {943--958},
}

@inproceedings{naka_optimization_2021,
	address = {New York, NY, USA},
	series = {{ICCDE} 2021},
	title = {Optimization techniques in data management: {A} survey},
	isbn = {978-1-4503-8845-0},
	url = {https://doi.org/10.1145/3456172.3456214},
	doi = {10.1145/3456172.3456214},
	abstract = {Data Management can be defined as the process of extracting, storing, organizing, and maintaining the\&nbsp;data\&nbsp;created and collected in organizations. Today's organizations invest in data management solutions that provide an efficient way to manage data in a unified structure. The enormously growth of data in the last decades has created a necessity for the fast extracting, accessing, and processing of the data. Optimization has been a key component in improving\&nbsp;the system's performance, searching and accessing data in different data management solutions. Optimization is a mathematical discipline that formulates mathematical models and finds the best solution among a set of feasible solutions. This paper aims to give a general overview of applications of optimization techniques and algorithms in different areas of data management in the last decades. Data management includes a large group of functionalities, but we will focus on studying and reviewing the recent development of optimization algorithms used in databases, data warehouses, big data and machine learning. Furthermore, this paper will identify applications of optimization in data management, reviews the current solutions proposed and emphasize future topics where there is a lack of studies in data management.},
	booktitle = {2021 7th international conference on computing and data engineering},
	publisher = {Association for Computing Machinery},
	author = {Naka, Edjola and Guliashki, Vassil},
	year = {2021},
	note = {Number of pages: 6
Place: Phuket, Thailand},
	keywords = {Big Data, Database, Machine Learning, Optimization},
	pages = {8--13},
}

@inproceedings{yong_optimizing_2019,
	address = {Cham},
	title = {Optimizing {Performance} of {Aggregate} {Query} {Processing} with {Histogram} {Data} {Structure}},
	isbn = {978-3-030-19807-7},
	doi = {10.1007/978-3-030-19807-7_33},
	abstract = {In today’s big data era, the capability of analyze massive data efficient and return the results within an short time limit is critical to decision making, thus many big data system proposed and various distributed and parallel processing techniques are heavily investigated. Among previous research, most of them are working on precise query processing, while approximate query processing (AQP) techniques which make interactive data exploration more efficiently and allows users to tradeoff between query accuracy and response time have not been investigate comprehensively. In this paper, we study the characteristics of aggregate query, a typical type of analytical query, and proposed an approximate query processing approach to optimize the execution of massive data based aggregate query with a histogram data structure. We implemented this approach into big data system Hive and compare it with Hive and AQP-enabled big data system BlinkDB, the experimental results verified that our approach is significantly fast than these existing systems in most scenarios.},
	booktitle = {Software {Engineering} {Methods} in {Intelligent} {Algorithms}},
	publisher = {Springer International Publishing},
	author = {Yong, Liang and Zhaonan, Mu},
	editor = {Silhavy, Radek},
	year = {2019},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {342--350},
}

@inproceedings{peng_one_2022,
	address = {New York, NY, USA},
	series = {{SIGMOD} '22},
	title = {One size does not fit all: {A} bandit-based sampler combination framework with theoretical guarantees},
	isbn = {978-1-4503-9249-5},
	url = {https://doi.org/10.1145/3514221.3517900},
	doi = {10.1145/3514221.3517900},
	abstract = {Sample-based estimation, which uses a sample to estimate population parameters (e.g., SUM, COUNT, and AVG), has various applications in database systems. A sampler defines how samples are drawn from a population. Various samplers have been proposed (e.g., uniform sampler, stratified sampler, and measure-biased sampler), since there is no single sampler that works well in all cases. To overcome the "one size does not fit all" challenge, we study how to combine multiple samplers to estimate population parameters, and propose SamComb, a novel bandit-based sampler combination framework. Given a set of samplers, a budget, and a population parameter, SamComb can automatically decide how much budget should be allocated to each sampler so that the combined estimation achieves the highest accuracy. We model this sampler combination problem as a multi-armed bandit (MAB) problem and propose effective approaches to balance the exploration and exploitation trade-off in a principled way. We provide theoretical guarantees for our approaches and conduct extensive experiments on both synthetic and real datasets. The results show that there is a strong need to combine multiple samplers, in order to obtain accurate estimations without the knowledge about population predicates and distributions, and SamComb is an effective framework to achieve this goal.},
	booktitle = {Proceedings of the 2022 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Peng, Jinglin and Ding, Bolin and Wang, Jiannan and Zeng, Kai and Zhou, Jingren},
	year = {2022},
	note = {Number of pages: 14
Place: Philadelphia, PA, USA},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {531--544},
}

@article{zhang_laqp_2021,
	title = {{LAQP}: {Learning}-based approximate query processing},
	volume = {546},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520309828},
	doi = {https://doi.org/10.1016/j.ins.2020.09.070},
	abstract = {The querying on big data is a challenging task due to the rapid growth of data amount. Approximate query processing (AQP) is a way to meet the requirement of fast response. In this paper, we propose a learning-based AQP method called the LAQP. The LAQP builds an error model learned from the historical queries to predict the sampling-based estimation error of each new query. It makes a combination of the sampling-based AQP, the pre-computed aggregations and the learned error model to provide high-accurate query estimations with a small off-line sample. The experimental results demonstrated that our LAQP outperforms the sampling-based AQP, the pre-aggregation-based AQP and the most recent learning-based AQP method.},
	journal = {Information Sciences},
	author = {Zhang, Meifan and Wang, Hongzhi},
	year = {2021},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1113--1134},
}

@inproceedings{bi_learningbased_2022,
	address = {Cham},
	title = {Learning-{Based} {Optimization} for {Online} {Approximate} {Query} {Processing}},
	isbn = {978-3-031-00123-9},
	doi = {10.1007/978-3-031-00123-9_7},
	abstract = {Approximate query processing (AQP) technique speeds up query execution by reducing the amount of data that needs to be processed, while sacrificing the accuracy of the query result to some extent. AQP is essentially a trade-off between the accuracy of the query result and the query latency. However, the heuristic AQP optimization and error control mechanism used by the existing AQP system fails to meet the accuracy requirements of users. This paper proposes a deep learning-based error prediction model to guide AQP query optimization. By using this model, we can estimate the errors of candidate query plans and select the appropriate plans that can meet the accuracy requirement with high probability. Extensive experiments show that the AQP system proposed in this paper can outperform the state-of-the-art online sampling-based AQP approach.},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	publisher = {Springer International Publishing},
	author = {Bi, Wenyuan and Zhang, Hanbing and Jing, Yinan and He, Zhenying and Zhang, Kai and Wang, X. Sean},
	editor = {Bhattacharya, Arnab and Lee Mong Li, Janice and Agrawal, Divyakant and Reddy, P. Krishna and Mohania, Mukesh and Mondal, Anirban and Goyal, Vikram and Uday Kiran, Rage},
	year = {2022},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {96--103},
}

@inproceedings{goyal_multiobjective_2022,
	address = {Singapore},
	title = {Multi-objective {Fuzzy}-{Swarm} {Optimizer} for {Data} {Partitioning}},
	isbn = {978-981-16-2164-2},
	doi = {10.1007/978-981-16-2164-2_25},
	abstract = {To boost the performance level of big data, data partitioning is considered to be as the backbone of big data applications. In recent years, many researchers are focusing their work toward data science and analysis for real-time applications with the integration of big data. Human interaction with data partitioning of big data is quite time-consuming. So, it is needed to make the data partition elastic as well as scalable while handling a high workload under the distributed system. In this paper, a multi-objective fuzzy-swarm optimization algorithm is proposed for cluster-based data partitioning. This paper also provided an analytical result analysis of different optimization algorithms for data partitioning, i.e., reduction or clustering along with their limitations. This paper provides an approach to enhance the efficiency level for clustering large complex data.},
	booktitle = {Advanced {Computing} and {Intelligent} {Technologies}},
	publisher = {Springer Singapore},
	author = {Goyal, S. B. and Bedi, Pradeep and Rajawat, Anand Singh and Shaw, Rabindra Nath and Ghosh, Ankush},
	editor = {Bianchini, Monica and Piuri, Vincenzo and Das, Sanjoy and Shaw, Rabindra Nath},
	year = {2022},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {307--318},
}

@inproceedings{liu_lhist_2021,
	title = {{LHist}: {Towards} learning multi-dimensional histogram for massive spatial data},
	doi = {10.1109/ICDE51399.2021.00107},
	abstract = {Data synopsis is widely adopted to speed-up query processing over large spatial databases. As one of the most popular spatial data synopses, multi-dimensional histograms (MH) have been studied and adopted by modern DBMS and analytical systems for decades. However, existing MH construction techniques highly rely on expert knowledge and statistical assumptions, making them hard to achieve consistently satisfactory performance across different datasets. Inspired by the emerging learned index techniques where the widely used index structures like B-tree can be further improved by integrating simple machine learning models, in this paper, we propose a learned data synopsis technique named Learned Multi-dimensional Histogram (LHist). Compared with the traditional data synopsis techniques, LHist is fully data-driven, easy-to-implement, and has the potential to achieve better storage-accuracy trade-off. On the typical task of range COUNT query estimation, the extensive experimental studies on large-scale real-world datasets and synthetic benchmarks reveal that LHist can outperform the existing synopsis structures in terms of storage cost, query processing efficiency, and estimation accuracy.},
	booktitle = {2021 {IEEE} 37th international conference on data engineering ({ICDE})},
	author = {Liu, Qiyu and Shen, Yanyan and Chen, Lei},
	month = apr,
	year = {2021},
	note = {ISSN: 2375-026X},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1188--1199},
}

@article{pahins_hashedcubes_2017,
	title = {Hashedcubes: {Simple}, low memory, real-time visual exploration of big data},
	volume = {23},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2016.2598624},
	abstract = {We propose Hashedcubes, a data structure that enables real-time visual exploration of large datasets that improves the state of the art by virtue of its low memory requirements, low query latencies, and implementation simplicity. In some instances, Hashedcubes notably requires two orders of magnitude less space than recent data cube visualization proposals. In this paper, we describe the algorithms to build and query Hashedcubes, and how it can drive well-known interactive visualizations such as binned scatterplots, linked histograms and heatmaps. We report memory usage, build time and query latencies for a variety of synthetic and real-world datasets, and find that although sometimes Hashedcubes offers slightly slower querying times to the state of the art, the typical query is answered fast enough to easily sustain a interaction. In datasets with hundreds of millions of elements, only about 2\% of the queries take longer than 40ms. Finally, we discuss the limitations of data structure, potential spacetime tradeoffs, and future research directions.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Pahins, Cícero A. L. and Stephens, Sean A. and Scheidegger, Carlos and Comba, João L. D.},
	month = jan,
	year = {2017},
	keywords = {RInteractive: Yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {671--680},
}

@article{xiong_geogap_2019,
	title = {Geo-gap tree: {A} progressive query and visualization method for massive spatial data},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2929531},
	abstract = {Online visualization and query of massive geo-spatial data are facing increasing challenges with the explosive growth of location-based spatial datasets. In the practical scenario, online visualization is carried out in a progressive way, namely, a sketchy view map is first presented, and more detailed view maps are produced gradually as the viewport scale goes deeper. One approach is to use the multi-scale spatial index technique. However, it loses the original data attribute and cannot provide spatial statistics information. The paper is to provide an improved index structure, the Geo-Gap tree, which aims to enhance online interactive access to large spatial datasets, as well as enable one to compute statistical attributes like aggregation at the coarse level. Therefore, the first focus of Geo-Gap tree is improving the efficiency of tree building. For this purpose, an adaptive geohash coding is introduced to reduce the computing of neighboring objects. And, this phase can be improved in parallel once objects are partitioned. Compare to Gap tree, the cost of building the Geo-Gap tree can be greatly reduced. The second contribution is to choose data at different level based on sampling so that a sample for each level can be served as a progressive query result. The third contribution is an estimation of progressive query results, which ensure that progressive query accuracy can be controlled within the range of theoretical analysis. With the query continuing to execute, the query results become more and more accurate. The method is now integrated successfully into a high-performance geographic information system called HiGIS.},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Xiong, Wei and Li, Ruiqing and Peng, Jin and Wu, Ye and Guo, Ning and Jing, Ning},
	year = {2019},
	keywords = {RInteractive: Yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {99428--99440},
}

@incollection{quoc_incremental_2019,
	address = {Cham},
	title = {Incremental {Approximate} {Computing}},
	isbn = {978-3-319-77525-8},
	url = {https://doi.org/10.1007/978-3-319-77525-8_151},
	abstract = {Approximate computing is increasingly used for speeding up computations and efficiently utilizing the computing resources. The idea behind approximate computing is to return an approximate answer instead of the exact answer for user queries. The trick is to choose a representative sample of the data for computing instead of using the entire data. As a result, it allows users to trade-off query accuracy for response time, enabling interactive queries over massive data by running queries on data samples and presenting results annotated with meaningful error bars. At the same time, another technique called incremental computing tries to achieve the same goals as approximate computing, i.e., speeding up job execution and utilizing resource efficiently. Incremental computing relies on the memoization of intermediate results of sub-computations and reusing these memoized results across jobs. This work makes the observation that these two computing paradigms are complementary and can be married together! The idea is quite simple: design a sampling algorithm that biases the sample selection to the memoized data items from previous runs. To realize this idea, an online stratified sampling algorithm is designed. The algorithm uses self-adjusting computation to produce an incrementally updated approximate output with bounded error. The algorithm is implemented in a data analytics system called IncApprox.},
	booktitle = {Encyclopedia of {Big} {Data} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Quoc, Do Le and Krishnan, Dhanya R. and Bhatotia, Pramod and Fetzer, Christof and Rodrigues, Rodrigo},
	editor = {Sakr, Sherif and Zomaya, Albert Y.},
	year = {2019},
	doi = {10.1007/978-3-319-77525-8_151},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1000--1007},
}

@article{han_efficiently_2018,
	title = {Efficiently processing deterministic approximate aggregation query on massive data},
	volume = {57},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-017-1136-z},
	doi = {10.1007/s10115-017-1136-z},
	abstract = {In actual applications, aggregation is an important operation to return statistical characterizations of subset of the data set. On massive data, approximate aggregation often is preferable for its better timeliness and responsiveness. This paper focuses on deterministic approximate aggregation to return running aggregate within progressive deterministic error interval. The existing methods either return approximate results with fixed accuracy, or return online approximate aggregate with probabilistic confidence interval, or incur a high I/O cost on massive data. This paper proposes LDA algorithm to compute deterministic approximate aggregate on massive data efficiently. LDA utilizes selection attribute lattice of hierarchical structure to distribute tuples and obtain a horizontal partitioning of the table. In each partition, each selection attribute is kept in column file and each ranking attribute is transposed to bit-slices. Given the selection condition, only relevant partitions are involved to compute the running aggregate. The compact storage scheme based on Z-order space filling curve is proposed to reduce the management cost of the partitions. An error reduction method is devised to reduce the error interval when computing running aggregate. The extensive experimental results on synthetic and real data sets show that LDA has a significant performance advantage over the existing algorithms.},
	number = {2},
	journal = {Knowledge and Information Systems},
	author = {Han, Xixian and Wang, Bailing and Li, Jianzhong and Gao, Hong},
	month = nov,
	year = {2018},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {437--473},
}

@inproceedings{lee_exploiting_2022,
	title = {Exploiting machine learning models for approximate query processing},
	doi = {10.1109/BigData55660.2022.10020252},
	abstract = {Approximate query processing can help reduce response time for aggregate queries in exploratory data analysis. In this study, we describe basic query transformation rules for processing approximate queries using synthetic data tables or inferential models. Based on the preliminary experimental results, we confirm that ML models can be used to provide approximate query results in response times acceptable for applications.},
	booktitle = {2022 {IEEE} international conference on big data (big data)},
	author = {Lee, Taewhi and Nam, Kihyuk and Park, Choon Seo and Kim, Sung-Soo},
	month = dec,
	year = {2022},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Validation Research},
	pages = {6752--6754},
}

@inproceedings{ma_dbest_2019,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {{DBEst}: {Revisiting} approximate query processing engines with machine learning models},
	isbn = {978-1-4503-5643-5},
	url = {https://doi.org/10.1145/3299869.3324958},
	doi = {10.1145/3299869.3324958},
	abstract = {In the era of big data, computing exact answers to analytical queries becomes prohibitively expensive. This greatly increases the value of approaches that can compute efficiently approximate, but highly-accurate, answers to analytical queries. Alas, the state of the art still suffers from many shortcomings: Errors are still high unless large memory investments are made. Many important analytics tasks are not supported. Query response times are too long and thus approaches rely on parallel execution of queries atop large big data analytics clusters, in-situ or in the cloud, whose acquisition/use costs dearly. Hence, the following questions are crucial: Can we develop AQP engines that reduce response times by orders of magnitude, ensure high accuracy, and support most aggregate functions? With smaller memory footprints and small overheads to build the state upon which they are based? With this paper, we show that the answers to all questions above can be positive. The paper presents DBEst, a system based on Machine Learning models (regression models and probability density estimators). It will discuss its limitations, promises, and how it can complement existing systems. It will substantiate its advantages using queries and data from the TPC-DS benchmark and real-life datasets, compared against state of the art AQP engines.},
	booktitle = {Proceedings of the 2019 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Ma, Qingzhi and Triantafillou, Peter},
	year = {2019},
	note = {Number of pages: 18
Place: Amsterdam, Netherlands},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1553--1570},
}

@inproceedings{tanted_database_2020,
	address = {New York, NY, USA},
	series = {{CoDS} {COMAD} 2020},
	title = {Database and caching support for adaptive visualization of large sensor data},
	isbn = {978-1-4503-7738-6},
	url = {https://doi.org/10.1145/3371158.3371170},
	doi = {10.1145/3371158.3371170},
	abstract = {Rapid deployment of Internet of Things (IoT) has led to ubiquitous and pervasive sensing of objects in the physical world, such as artifacts in buildings, agriculture, cities, the electric grid, etc. Meaningful visualization of large amounts of sensor data demands user-friendly, convenient and flexible tools. In this paper, we discuss the design, implementation and performance of a novel distributed caching \& aggregation mechanism to handle the visualization of sensor data, which is time series data. Its features include a) bitmap indexing for capturing the dynamics of the cached data b) exploiting recency of data usage when making cache insertion and replacement decisions and c) integrating existing databases and open-source visualization platforms to provide quick and effective distributed caching solutions to handle time-series data. We evaluate our system on real-world data generated by sensors deployed in an academic building and demonstrate empirically that the system adapts to evolving workload patterns and makes it attractive for a variety of workloads.},
	booktitle = {Proceedings of the 7th {ACM} {IKDD} {CoDS} and 25th {COMAD}},
	publisher = {Association for Computing Machinery},
	author = {Tanted, Sapan and Agarwal, Anshul and Mitra, Shinjan and Bahuman, Chaitra and Ramamritham, Krithi},
	year = {2020},
	note = {Number of pages: 9
Place: Hyderabad, India},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {98--106},
}

@inproceedings{liang_combining_2021,
	address = {New York, NY, USA},
	series = {{SIGMOD} '21},
	title = {Combining aggregation and sampling (nearly) optimally for approximate query processing},
	isbn = {978-1-4503-8343-1},
	url = {https://doi.org/10.1145/3448016.3457277},
	doi = {10.1145/3448016.3457277},
	abstract = {Sample-based approximate query processing (AQP) suffers from many pitfalls such as the inability to answer very selective queries and unreliable confidence intervals when sample sizes are small. Recent research presented an intriguing solution of combining materialized, pre-computed aggregates with sampling for accurate and more reliable AQP. We explore this solution in detail in this work and propose an AQP physical design called PASS, or Precomputation-Assisted Stratified Sampling. PASS builds a tree of partial aggregates that cover different partitions of the dataset. The leaf nodes of this tree form the strata for stratified samples. Aggregate queries whose predicates align with the partitions (or unions of partitions) are exactly answered with a depth-first search, and any partial overlaps are approximated with the stratified samples. We propose an algorithm for optimally partitioning the data into such a data structure with various practical approximation techniques.},
	booktitle = {Proceedings of the 2021 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Liang, Xi and Sintos, Stavros and Shang, Zechao and Krishnan, Sanjay},
	year = {2021},
	note = {Number of pages: 13
Place: Virtual Event, China},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1129--1141},
}

@inproceedings{mohammed_continuous_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {Continuous prefetch for interactive data applications},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3384405},
	doi = {10.1145/3318464.3384405},
	abstract = {Interactive data visualization and exploration (DVE) applications, such as the one in Figure 1, have rapidly grown in popularity with use cases in numerous sectors [2, 4, 9, 11, 15]. Like typical web services, DVE applications may be run on heterogeneous client devices and networks, with users expecting fast response times under 100 ms [12]. However, the resource demands of DVE applications are magnified and highly unpredictable, making it difficult to achieve such interactivity.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Mohammed, Haneen},
	year = {2020},
	note = {Number of pages: 3
Place: Portland, OR, USA},
	keywords = {RInteractive: Yes, UserStudy: Yes, cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2841--2843},
}

@inproceedings{park_blinkml_2019,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {{BlinkML}: {Efficient} maximum likelihood estimation with probabilistic guarantees},
	isbn = {978-1-4503-5643-5},
	url = {https://doi.org/10.1145/3299869.3300077},
	doi = {10.1145/3299869.3300077},
	abstract = {The rising volume of datasets has made training machine learning (ML) models a major computational cost in the enterprise. Given the iterative nature of model and parameter tuning, many analysts use a small sample of their entire data during their initial stage of analysis to make quick decisions (e.g., what features or hyperparameters to use) and use the entire dataset only in later stages (i.e., when they have converged to a specific model). This sampling, however, is performed in an ad-hoc fashion. Most practitioners cannot precisely capture the effect of sampling on the quality of their model, and eventually on their decision-making process during the tuning phase. Moreover, without systematic support for sampling operators, many optimizations and reuse opportunities are lost. In this paper, we introduce BlinkML, a system for fast, quality-guaranteed ML training. BlinkML allows users to make error-computation tradeoffs: instead of training a model on their full data (i.e., full model), BlinkML can quickly train an approximate model with quality guarantees using a sample. The quality guarantees ensure that, with high probability, the approximate model makes the same predictions as the full model. BlinkML currently supports any ML model that relies on maximum likelihood estimation (MLE), which includes Generalized Linear Models (e.g., linear regression, logistic regression, max entropy classifier, Poisson regression) as well as PPCA (Probabilistic Principal Component Analysis). Our experiments show that BlinkML can speed up the training of large-scale ML tasks by 6.26×?629× while guaranteeing the same predictions, with 95\% probability, as the full model.},
	booktitle = {Proceedings of the 2019 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Park, Yongjoo and Qing, Jingyi and Shen, Xiaoyang and Mozafari, Barzan},
	year = {2019},
	note = {Number of pages: 18
Place: Amsterdam, Netherlands},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1135--1152},
}

@article{li_bounded_2019,
	title = {Bounded approximate query processing},
	volume = {31},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2018.2877362},
	abstract = {OLAP is a core functionality in database systems and the performance is crucial to enable on-time decisions. However, OLAP queries are rather time consuming, especially on large datasets, and traditional exact solutions usually cannot meet the high-performance requirement. Recently, approximate query processing (AQP) has been proposed to enable approximate OLAP. However, existing AQP methods have some limitations. First, they may involve unacceptable errors on skewed data (e.g., long-tail distribution). Second, they require to store large amount of data and have no significant performance improvement. Third, they only support a small subset of SQL aggregation queries. To overcome these limitations, we propose a bounded approximate query processing framework BAQ. Given a predefined error bound and a set of queries, BAQ judiciously selects high-quality samples from the data to generate a unified synopsis offline, and then uses the synopsis to answer online queries. Compared with existing methods, BAQ has the following salient features. (1) BAQ does not need to generate a synopsis for each query while it only generates a unified synopsis, and thus BAQ has much smaller synopsis. (2) BAQ achieves much smaller error than existing studies. Specifically, BAQ can provide deterministic approximate results (i.e., the estimated query results must be within the error bound with 100 percent confidence) for SQL aggregation queries that do not contain selection conditions on numerical columns. For queries with selection conditions on numerical columns, we propose effective grouping-based techniques and the estimated results are also within the error bound in practice. Experimental results on both real and synthetic datasets show that BAQ significantly outperforms state-of-the-art approaches. For example, on a Microsoft production dataset (a real dataset with synthetic queries), BAQ has 10-100× improvement on synopsis size and 10-100× improvement on the error compared with state-of-the-art algorithms.},
	number = {12},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Kaiyu and Zhang, Yong and Li, Guoliang and Tao, Wenbo and Yan, Ying},
	month = dec,
	year = {2019},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2262--2276},
}

@article{han_clap_2017,
	title = {{CLAP}: {Component}-level approximate processing for low tail latency and high result accuracy in cloud online services},
	volume = {28},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2017.2650988},
	abstract = {Modern latency-critical online services such as search engines often process requests by consulting large input data spanning massive parallel components. Hence the tail latency of these components determines the service latency. To trade off result accuracy for tail latency reduction, existing techniques use the components responding before a specified deadline to produce approximate results. However, they skip a large proportion of components when load gets heavier, thus incurring large accuracy losses. In this paper, we propose CLAP to enable component-level approximate processing of requests for low tail latency and small accuracy losses. CLAP aggregates information of input data to create small aggregated data points. Using these points, CLAP reduces latency variance of parallel components and allows them to produce initial results quickly; CLAP also identifies the parts of input data most related to requests' result accuracies, thus first using these parts to improve the produced results to minimize accuracy losses. We evaluated CLAP using real services and datasets. The results show: (i) CLAP reduces tail latency by 6.46 times with accuracy losses of 2.2 percent compared to existing exact processing techniques; (ii) when using the same latency, CLAP reduces accuracy losses by 31.58 times compared to existing approximate processing techniques.},
	number = {8},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Han, Rui and Huang, Siguang and Wang, Zhentao and Zhan, Jianfeng},
	month = aug,
	year = {2017},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2190--2203},
}

@inproceedings{xia_crossindex_2022,
	address = {Cham},
	title = {{CrossIndex}: {Memory}-{Friendly} and {Session}-{Aware} {Index} for {Supporting} {Crossfilter} in {Interactive} {Data} {Exploration}},
	isbn = {978-3-031-00123-9},
	doi = {10.1007/978-3-031-00123-9_38},
	abstract = {Crossfilter, a typical application for interactive data exploration (IDE), is widely used in data analysis, BI, and other fields. However, with the scale-up of the dataset, the real-time response of crossfilter can be hardly fulfilled. In this paper, we propose a memory-friendly and session-aware index called CrossIndex, which can support crossfilter-style queries with low latency. We first analyze a large number of query workloads generated by previous work and find that queries in the data exploration workload are inter-dependent, which means these queries have overlapped predicates. Based on this observation, this paper defines the inter-dependent queries as a session and builds a hierarchical index that can be used to accelerate crossfilter-style query processing by utilizing the overlapped property of the session to reduce unnecessary search space. Extensive experiments show that CrossIndex outperforms almost all other approaches and meanwhile keeps a low building cost.},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	publisher = {Springer International Publishing},
	author = {Xia, Tianyu and Zhang, Hanbing and Jing, Yinan and He, Zhenying and Zhang, Kai and Wang, X. Sean},
	editor = {Bhattacharya, Arnab and Lee Mong Li, Janice and Agrawal, Divyakant and Reddy, P. Krishna and Mohania, Mukesh and Mondal, Anirban and Goyal, Vikram and Uday Kiran, Rage},
	year = {2022},
	keywords = {RInteractive: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {476--492},
}

@inproceedings{lin_bigin4_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {{BigIN4}: {Instant}, interactive insight identification for multi-dimensional big data},
	isbn = {978-1-4503-5552-0},
	url = {https://doi.org/10.1145/3219819.3219867},
	doi = {10.1145/3219819.3219867},
	abstract = {The ability to identify insights from multi-dimensional big data is important for business intelligence. To enable interactive identification of insights, a large number of dimension combinations need to be searched and a series of aggregation queries need to be quickly answered. The existing approaches answer interactive queries on big data through data cubes or approximate query processing. However, these approaches can hardly satisfy the performance or accuracy requirements for ad-hoc queries demanded by interactive exploration. In this paper, we present BigIN4, a system for instant, interactive identification of insights from multi-dimensional big data. BigIN4 gives insight suggestions by enumerating subspaces and answers queries by combining data cube and approximate query processing techniques. If a query cannot be answered by the cubes, BigIN4 decomposes it into several low dimensional queries that can be directly answered by the cubes through an online constructed Bayesian Network and gives an approximate answer within a statistical interval. Unlike the related works, BigIN4 does not require any prior knowledge of queries and does not assume a certain data distribution. Our experiments on ten real-world large-scale datasets show that BigIN4 can successfully identify insights from big data. Furthermore, BigIN4 can provide approximate answers to aggregation queries effectively (with less than 10\% error on average) and efficiently (50x faster than sampling-based methods).},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} international conference on knowledge discovery \&amp; data mining},
	publisher = {Association for Computing Machinery},
	author = {Lin, Qingwei and Ke, Weichen and Lou, Jian-Guang and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Zhou, Ziyi and Qiao, Bo and Zhang, Dongmei},
	year = {2018},
	note = {Number of pages: 9
Place: London, United Kingdom},
	keywords = {RInteractive: Yes, UserStudy: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {547--555},
}

@inproceedings{dong_arrayudf_2017,
	address = {New York, NY, USA},
	series = {{HPDC} '17},
	title = {{ArrayUDF}: {User}-defined scientific data analysis on arrays},
	isbn = {978-1-4503-4699-3},
	url = {https://doi.org/10.1145/3078597.3078599},
	doi = {10.1145/3078597.3078599},
	abstract = {User-Defined Functions (UDF) allow application programmers to specify analysis operations on data, while leaving the data management tasks to the system. This general approach enables numerous custom analysis functions and is at the heart of the modern Big Data systems. Even though the UDF mechanism can theoretically support arbitrary operations, a wide variety of common operations – such as computing the moving average of a time series, the vorticity of a fluid flow, etc., – are hard to express and slow to execute. Since these operations are traditionally performed on multi-dimensional arrays, we propose to extend the expressiveness of structural locality for supporting UDF operations on arrays. We further propose an in situ UDF mechanism, called ArrayUDF, to implement the structural locality. ArrayUDF allows users to define computations on adjacent array cells without the use of join operations and executes the UDF directly on arrays stored in data files without requiring to load their content into a data management system. Additionally, we present a thorough theoretical analysis of the data access cost to exploit the structural locality, which enables ArrayUDF to automatically select the best array partitioning strategy for a given UDF operation. In a series of performance evaluations on large scientific datasets, we have observed that – using the generic UDF interface – ArrayUDF consistently outperforms Spark, SciDB, and RasDaMan.},
	booktitle = {Proceedings of the 26th international symposium on high-performance parallel and distributed computing},
	publisher = {Association for Computing Machinery},
	author = {Dong, Bin and Wu, Kesheng and Byna, Surendra and Liu, Jialin and Zhao, Weijie and Rusu, Florin},
	year = {2017},
	note = {Number of pages: 12
Place: Washington, DC, USA},
	keywords = {RDistributed: Yes, RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {53--64},
}

@inproceedings{personnaz_balancing_2021,
	address = {New York, NY, USA},
	series = {{aiDM} '21},
	title = {Balancing familiarity and curiosity in data exploration with deep reinforcement learning},
	isbn = {978-1-4503-8535-0},
	url = {https://doi.org/10.1145/3464509.3464884},
	doi = {10.1145/3464509.3464884},
	abstract = {The ability to find a set of records in Exploratory Data Analysis (EDA) hinges on the scattering of objects in the data set and the on users’ knowledge of data and their ability to express their needs. This yields a wide range of EDA scenarios and solutions that differ in the guidance they provide to users. In this paper, we investigate the interplay between modeling curiosity and familiarity in Deep Reinforcement Learning (DRL) and expressive data exploration operators. We formalize curiosity as intrinsic reward and familiarity as extrinsic reward. We examine the behavior of several policies learned for different weights for those rewards. Our experiments on SDSS, a very large sky survey data set1 provide several insights and justify the need for a deeper examination of combining DRL and data exploration operators that go beyond drill-downs and roll-ups.},
	booktitle = {Fourth workshop in exploiting {AI} techniques for data management},
	publisher = {Association for Computing Machinery},
	author = {Personnaz, Aurélien and Amer-Yahia, Sihem and Berti-Equille, Laure and Fabricius, Maximilian and Subramanian, Srividya},
	year = {2021},
	note = {Number of pages: 8
Place: Virtual Event, China},
	keywords = {cluster:Automatic Exploration, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {16--23},
}

@inproceedings{shi_are_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Are we building on the rock? {On} the importance of data preprocessing for code summarization},
	isbn = {978-1-4503-9413-0},
	url = {https://doi.org/10.1145/3540250.3549145},
	doi = {10.1145/3540250.3549145},
	abstract = {Code summarization, the task of generating useful comments given the code, has long been of interest. Most of the existing code summarization models are trained and validated on widely-used code comment benchmark datasets. However, little is known about the quality of the benchmark datasets built from real-world projects. Are the benchmark datasets as good as expected? To bridge the gap, we conduct a systematic research to assess and improve the quality of four benchmark datasets widely used for code summarization tasks. First, we propose an automated code-comment cleaning tool that can accurately detect noisy data caused by inappropriate data preprocessing operations from existing benchmark datasets. Then, we apply the tool to further assess the data quality of the four benchmark datasets, based on the detected noises. Finally, we conduct comparative experiments to investigate the impact of noisy data on the performance of code summarization models. The results show that these data preprocessing noises widely exist in all four benchmark datasets, and removing these noisy data leads to a significant improvement on the performance of code summarization. We believe that the findings and insights will enable a better understanding of data quality in code summarization tasks, and pave the way for relevant research and practice.},
	booktitle = {Proceedings of the 30th {ACM} joint european software engineering conference and symposium on the foundations of software engineering},
	publisher = {Association for Computing Machinery},
	author = {Shi, Lin and Mu, Fangwen and Chen, Xiao and Wang, Song and Wang, Junjie and Yang, Ye and Li, Ge and Xia, Xin and Wang, Qing},
	year = {2022},
	note = {Number of pages: 13
Place: Singapore, Singapore},
	keywords = {Code Summarization, Data Quality, Empirical Study},
	pages = {107--119},
}

@inproceedings{peng_aqp_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {{AQP}++: {Connecting} approximate query processing with aggregate precomputation for interactive analytics},
	isbn = {978-1-4503-4703-7},
	url = {https://doi.org/10.1145/3183713.3183747},
	doi = {10.1145/3183713.3183747},
	abstract = {Interactive analytics requires database systems to be able to answer aggregation queries within interactive response times. As the amount of data is continuously growing at an unprecedented rate, this is becoming increasingly challenging. In the past, the database community has proposed two separate ideas, sampling-based approximate query processing (AQP) and aggregate precomputation (AggPre) such as data cubes, to address this challenge. In this paper, we argue for the need to connect these two separate ideas for interactive analytics. We propose AQP++, a novel framework to enable the connection. The framework can leverage both a sample as well as a precomputed aggregate to answer user queries. We discuss the advantages of having such a unified framework and identify new challenges to fulfill this vision. We conduct an in-depth study of these challenges for range queries and explore both optimal and heuristic solutions to address them. Our experiments using two public benchmarks and one real-world dataset show that AQP++ achieves a more flexible and better trade-off among preprocessing cost, query response time, and answer quality than AQP or AggPre.},
	booktitle = {Proceedings of the 2018 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Peng, Jinglin and Zhang, Dongxiang and Wang, Jiannan and Pei, Jian},
	year = {2018},
	note = {Number of pages: 16
Place: Houston, TX, USA},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1477--1492},
}

@inproceedings{quoc_approxjoin_2018,
	address = {New York, NY, USA},
	series = {{SoCC} '18},
	title = {{ApproxJoin}: {Approximate} distributed joins},
	isbn = {978-1-4503-6011-1},
	url = {https://doi.org/10.1145/3267809.3267834},
	doi = {10.1145/3267809.3267834},
	abstract = {A distributed join is a fundamental operation for processing massive datasets in parallel. Unfortunately, computing an equi-join over such datasets is very resource-intensive, even when done in parallel. Given this cost, the equi-join operator becomes a natural candidate for optimization using approximation techniques, which allow users to trade accuracy for latency. Finding the right approximation technique for joins, however, is a challenging task. Sampling, in particular, cannot be directly used in joins; naïvely performing a join over a sample of the dataset will not preserve statistical properties of the query result.To address this problem, we introduce ApproxJoin. We interweave Bloom filter sketching and stratified sampling with the join computation in a new operator that preserves statistical properties of an aggregation over the join output. ApproxJoin leverages Bloom filters to avoid shuffling non-joinable data items around the network, and then applies stratified sampling to obtain a representative sample of the join output. We implemented ApproxJoin in Apache Spark, and evaluated it using microbenchmarks and real-world workloads. Our evaluation shows that ApproxJoin scales well and significantly reduces data movement, without sacrificing tight error bounds on the accuracy of the final results. ApproxJoin achieves a speedup of up to 9x over unmodified Spark-based joins with the same sampling ratio. Furthermore, the speedup is accompanied by a significant reduction in the shuffled data volume, which is up to 82x less than unmodified Spark-based joins.},
	booktitle = {Proceedings of the {ACM} symposium on cloud computing},
	publisher = {Association for Computing Machinery},
	author = {Quoc, Do Le and Akkus, Istemi Ekin and Bhatotia, Pramod and Blanas, Spyros and Chen, Ruichuan and Fetzer, Christof and Strufe, Thorsten},
	year = {2018},
	note = {Number of pages: 13
Place: Carlsbad, CA, USA},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {426--438},
}

@inproceedings{bruhwiler_aperture_2019,
	address = {New York, NY, USA},
	series = {{UCC}'19},
	title = {Aperture: {Fast} visualizations over spatiotemporal datasets},
	isbn = {978-1-4503-6894-0},
	url = {https://doi.org/10.1145/3344341.3368817},
	doi = {10.1145/3344341.3368817},
	abstract = {One of the most powerful ways to explore data is to visualize it. Visualizations underpin data wrangling, feature space explorations, and understanding the dynamics of phenomena. Here, we explore interactive visualizations of voluminous, spatiotemporal datasets. Our system, Aperture, makes novel use of data sketches to reconcile I/O overheads, in particular the speed differential across the memory hierarchy, and data volumes. Queries underpin several aspects of our methodology. This includes support for a diversity of queries that are aligned with the construction of visual artifacts, facilitating their effective evaluation over the server (distributed) backend, and generating speculative queries based on a user's exploration trajectory. Aperture includes support for different visual artifacts, animations, and multilinked views via scalable brushing-and-linking. Finally, we also explore issues in effective containerization to support visualization workloads. Our empirical benchmarks profile several aspects of visualization performance and demonstrate the suitability of our methodology.},
	booktitle = {Proceedings of the 12th {IEEE}/{ACM} international conference on utility and cloud computing},
	publisher = {Association for Computing Machinery},
	author = {Bruhwiler, Kevin and Pallickara, Shrideep},
	year = {2019},
	note = {Number of pages: 10
Place: Auckland, New Zealand},
	keywords = {RInteractive: Yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {31--40},
}

@inproceedings{wu_aqapprox_2020,
	address = {Cham},
	title = {{AQapprox}: {Aggregation} {Queries} {Approximation} with {Distribution}-{Aware} {Online} {Sampling}},
	isbn = {978-3-030-62008-0},
	doi = {10.1007/978-3-030-62008-0_28},
	abstract = {Approximate query processing (AQP) is an effective way to provide approximate results for SQL queries, which relaxing accuracy in exchange for higher processing speed. In sampling-based AQP techniques, random sampling works well for uniformly distributed data but performs poorly on skewed data. To address this problem, we propose a distribution-aware approximation framework called AQapprox (aggregation queries approximation), to approximate queries more efficiently and accurately by extending Sapprox. We construct a probabilistic Map, which records the occurrences of sub-datasets on categorical columns and related statistics on numerical columns at each segment of the whole dataset. When a query arrives, AQapprox will combine Map and adaptively use different sampling methods based on the distribution. Experimental results on both real and synthetic datasets show that AQapprox can achieve a speedup by up to 5.9\$\${\textbackslash}times \$\$×for skewed data, 64\$\${\textbackslash}times \$\$×for uniform data over Sapprox, and has higher accuracy on multi-column queries.},
	booktitle = {Web {Information} {Systems} {Engineering} – {WISE} 2020},
	publisher = {Springer International Publishing},
	author = {Wu, Han and Wang, Xiaoling and Lu, Xingjian},
	editor = {Huang, Zhisheng and Beek, Wouter and Wang, Hua and Zhou, Rui and Zhang, Yanchun},
	year = {2020},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {404--416},
}

@inproceedings{regev_approximating_2021,
	title = {Approximating aggregated {SQL} queries with {LSTM} networks},
	doi = {10.1109/IJCNN52387.2021.9533974},
	abstract = {Despite continuous investments in data technologies, the latency of querying data still poses a significant challenge. Modern analytic solutions require near real-time responsiveness both to make them interactive and to support automated processing. Current technologies (Hadoop, Spark, Dataflow) scan the dataset to execute queries and focus on providing scalable data storage and in-memory concurrent data processing to maximize task execution speed. We argue that these solutions fail to offer an adequate level of interactivity, since they depend on continual access to data. In this paper, we present a method for query approximation, also known as approximate query processing (AQP), that reduces the need to scan data during inference (query calculation), thus enabling a rapid query processing tool. We use an LSTM network to learn the relationship between queries and their results, and to provide a rapid inference layer for the prediction of query results. Our method (referred to as “Hunch”) produces a lightweight LSTM network which provides high query throughput. We evaluated our method using 12 datasets and compared it to state-of-the-art AQP engines (VerdictDB, BlinkDB) in terms of the query latency, model weight, and accuracy. The results show that our method predicted query results with a normalized root mean squared error (NRMSE) ranging from approximately 1\% to 4\%, which, for the majority of our datasets, was better than the results of the benchmarks. Moreover, our method was able to predict up to 120,000 queries in a second (streamed together) and with a single query latency of no more than 2 ms.},
	booktitle = {2021 international joint conference on neural networks ({IJCNN})},
	author = {Regev, Nir and Rokach, Lior and Shabtai, Asaf},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1--8},
}

@inproceedings{muniswamaiah_approximate_2020,
	title = {Approximate query processing for big data in heterogeneous databases},
	doi = {10.1109/BigData50022.2020.9378310},
	abstract = {Big Data analytics is used in decision making. It involves heavy computation to obtain exact answers. To alleviate this problem, approximate query processing (AQP) was adopted, which provides approximate results with error bounds. The AQP models which have been proposed are supported only by a single database. In an organization, big data is stored in multiple databases that have different data models. This research aims to provide AQP as a middleware solution using query optimization for heterogeneous databases.},
	booktitle = {2020 {IEEE} international conference on big data (big data)},
	author = {Muniswamaiah, Manoj and Agerwala, Tilak and Tappert, Charles C.},
	month = dec,
	year = {2020},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {5765--5767},
}

@inproceedings{thirumuruganathan_approximate_2020,
	title = {Approximate query processing for data exploration using deep generative models},
	doi = {10.1109/ICDE48307.2020.00117},
	abstract = {Data is generated at an unprecedented rate surpassing our ability to analyze them. The database community has pioneered many novel techniques for Approximate Query Processing (AQP) that could give approximate results in a fraction of time needed for computing exact results. In this work, we explore the usage of deep learning (DL) for answering aggregate queries specifically for interactive applications such as data exploration and visualization. We use deep generative models, an unsupervised learning based approach, to learn the data distribution faithfully such that aggregate queries could be answered approximately by generating samples from the learned model. The model is often compact - few hundred KBs - so that arbitrary AQP queries could be answered on the client side without contacting the database server. Our other contributions include identifying model bias and minimizing it through a rejection sampling based approach and an algorithm to build model ensembles for AQP for improved accuracy. Our extensive experiments show that our proposed approach can provide answers with high accuracy and low latency.},
	booktitle = {2020 {IEEE} 36th international conference on data engineering ({ICDE})},
	author = {Thirumuruganathan, Saravanan and Hasan, Shohedul and Koudas, Nick and Das, Gautam},
	month = apr,
	year = {2020},
	note = {ISSN: 2375-026X},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1309--1320},
}

@inproceedings{lepekhin_systematic_2019,
	series = {{SERP4IoT} '19},
	title = {A systematic mapping study on internet of things challenges},
	url = {https://doi.org/10.1109/SERP4IoT.2019.00009},
	doi = {10.1109/SERP4IoT.2019.00009},
	abstract = {The challenge of developing IoT-based systems has been found to be a complex problem It is influenced by number of factors: heterogeneous devices/resources, various perception-action cycles and widely distributed devices and computing resources. Increasing complexity and immaturity to deal with it have resulted in growing range of problems and challenges in IoT development. This paper identifies essential IoT-related challenges by conducting a systematic mapping study of existing IoT literature. To this end, we distil information with respect to IoT-related: 1) challenges, 2) experimental studies, and 3) recommendations for future research. We then discuss our findings in order to understand better the general state of IoT research, potential gaps in research, and implications for future research.},
	booktitle = {Proceedings of the 1st international workshop on software engineering research \&amp; practices for the internet of things},
	publisher = {IEEE Press},
	author = {Lepekhin, Aleksandr and Borremans, Alexandra and Ilin, Igor and Jantunen, Sami},
	year = {2019},
	note = {Place: Montreal, Quebec, Canada
Number of pages: 8},
	keywords = {IoT, IoT challenges, IoT development, internet of things, systematic mapping study},
	pages = {9--16},
}

@inproceedings{villarreal-narvaez_systematic_2020,
	address = {New York, NY, USA},
	series = {{DIS} '20},
	title = {A systematic review of gesture elicitation studies: {What} can we learn from 216 studies?},
	isbn = {978-1-4503-6974-9},
	url = {https://doi.org/10.1145/3357236.3395511},
	doi = {10.1145/3357236.3395511},
	abstract = {Gesture elicitation studies represent a popular and resourceful method in HCI to inform the design of intuitive gesture commands, reflective of end-users' behavior, for controlling all kinds of interactive devices, applications, and systems. In the last ten years, an impressive body of work has been published on this topic, disseminating useful design knowledge regarding users' preferences for finger, hand, wrist, arm, head, leg, foot, and whole-body gestures. In this paper, we deliver a systematic literature review of this large body of work by summarizing the characteristics and findings ofN=216gesture elicitation studies subsuming 5,458 participants, 3,625 referents, and 148,340 elicited gestures. We highlight the descriptive, comparative, and generative virtues of our examination to provide practitioners with an effective method to (i) understand how new gesture elicitation studies position in the literature; (ii) compare studies from different authors; and (iii) identify opportunities for new research. We make our large corpus of papers accessible online as a Zotero group library at https://www.zotero.org/groups/2132650/gestureₑlicitationₛtudies.},
	booktitle = {Proceedings of the 2020 {ACM} designing interactive systems conference},
	publisher = {Association for Computing Machinery},
	author = {Villarreal-Narvaez, Santiago and Vanderdonckt, Jean and Vatavu, Radu-Daniel and Wobbrock, Jacob O.},
	year = {2020},
	note = {Number of pages: 18
Place: Eindhoven, Netherlands},
	keywords = {gesture elicitation, survey, systematic literature review},
	pages = {855--872},
}

@inproceedings{salloum_samplingbased_2019,
	address = {New York, NY, USA},
	series = {{CIKM} '19},
	title = {A sampling-based system for approximate big data analysis on computing clusters},
	isbn = {978-1-4503-6976-3},
	url = {https://doi.org/10.1145/3357384.3358124},
	doi = {10.1145/3357384.3358124},
	abstract = {To break the in-memory bottleneck and facilitate online sampling in cluster computing frameworks, we propose a new sampling-based system for approximate big data analysis on computing clusters. We address both computational and statistical aspects of big data across the main layers of cluster computing frameworks: big data storage, big data management, big data online sampling, big data processing, and big data exploration and analysis. We use the new Random Sample Partition (RSP) distributed data model to store a big data set as a set of ready-to-use random sample data blocks in Hadoop Distributed File System (HDFS), called RSP blocks. With this system, only a few RSP blocks are selected and processed using a sequential algorithm in a distributed data-parallel manner to produce approximate results for the entire data set. In this paper, we present a prototype RSP-based system and demonstrate its advantages. Our experiments show that RSP blocks can be used to get approximate models and summary statistics as well as estimate the proportions of inconsistent values without computing the entire data or running expensive online sampling operations. This new system enables big data exploration and analysis where the entire data set cannot be computed.},
	booktitle = {Proceedings of the 28th {ACM} international conference on information and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Salloum, Salman and Wu, Yinxu and Huang, Joshua Zhexue},
	year = {2019},
	note = {Number of pages: 4
Place: Beijing, China},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {2481--2484},
}

@inproceedings{savva_aggregate_2019,
	title = {Aggregate query prediction under dynamic workloads},
	doi = {10.1109/BigData47090.2019.9006267},
	abstract = {Large organizations have seamlessly incorporated data-driven decision making in their operations. However, as data volumes increase, expensive big data infrastructures are called to rescue. In this setting, analytics tasks become very costly in terms of query response time, resource consumption, and money in cloud deployments, especially when base data are stored across geographically distributed data centers. Therefore, we introduce an adaptive Machine Learning mechanism which is light-weight, stored client-side, can estimate the answers of a variety of aggregate queries and can avoid the big data backend. The estimations are performed in milliseconds and are inexepensive as the mechanism learns from past analytical-query patterns. However, as analytic queries are ad-hoc and analysts' interests change over time we develop solutions that can swiftly and accurately detect such changes and adapt to new query patterns. The capabilities of our approach are demonstrated using extensive evaluation with real and synthetic datasets.},
	booktitle = {2019 {IEEE} international conference on big data (big data)},
	author = {Savva, Fotis and Anagnostopoulos, Christos and Triantafillou, Peter},
	month = dec,
	year = {2019},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {671--676},
}

@inproceedings{wang_aggregate_2022,
	title = {Aggregate queries on knowledge graphs: {Fast} approximation with semantic-aware sampling},
	doi = {10.1109/ICDE53745.2022.00263},
	abstract = {A knowledge graph (KG) manages large-scale and real-world facts as a big graph in a schema-flexible manner. Aggregate query is a fundamental query over KGs, e.g., “what is the average price of cars produced in Germany?”. Despite its importance, answering aggregate queries on KGs has received little attention in the literature. Aggregate queries can be supported based on factoid queries, e.g., “find all cars produced in Germany”, by applying an additional aggregate operation on factoid queries' answers. However, this straightforward method is challenging because both the accuracy and efficiency of factoid query processing will seriously impact the performance of aggregate queries. In this paper, we propose a “sampling-estimation” model to answer aggregate queries over KGs, which is the first work to provide an approximate aggregate result with an effective accuracy guarantee, and without relying on factoid queries. Specifically, we first present a semantic-aware sampling to collect a high-quality random sample through a random walk based on knowledge graph embedding. Then, we propose unbiased estimators for COUNT, SUM, and a consistent estimator for AVG to compute the approximate aggregate results based on the random sample, with an accuracy guarantee in the form of confidence interval. We extend our approach to support iterative improvement of accuracy, and more complex queries with filter, GROUP-BY, and different graph shapes, e.g., chain, cycle, star, flower. Extensive experiments over real-world KGs demonstrate the effectiveness and efficiency of our approach.},
	booktitle = {2022 {IEEE} 38th international conference on data engineering ({ICDE})},
	author = {Wang, Yuxiang and Khan, Arijit and Xu, Xiaoliang and Jin, Jiahui and Hong, Qifan and Fu, Tao},
	month = may,
	year = {2022},
	note = {ISSN: 2375-026X},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {2914--2927},
}

@inproceedings{guyot_formal_2022,
	address = {New York, NY, USA},
	series = {{IDEAS} '22},
	title = {A formal framework for data lakes based on category theory},
	isbn = {978-1-4503-9709-4},
	url = {https://doi.org/10.1145/3548785.3548797},
	doi = {10.1145/3548785.3548797},
	abstract = {The management of Big Data requires flexible systems to handle the heterogeneity of data models as well as the complexity of analytical workflows. Traditional systems like data warehouses have reached their limits due to their rigid schema-on-write paradigm, that requires well identified and defined use cases to ingest data. Data lakes, with their schema-on-read paradigm, have been proposed as more flexible systems in which raw data are directly stored in their original format associated with metadata, to be accessed and transformed only when users need to process or analyze them. Thus, it is necessary to define and control the different levels of abstraction and the dependencies among functionalities of a data lake to use it efficiently. In this article, we present a formal framework aiming to define a data lake pattern and to unify the interactions among the functionalities. We use the category theory as theoretical foundations to benefit from its high level of abstraction and its compositionality. By relying on different categories and functors, we ensure the navigation among the functionalities and allow the composition of multiples operations, while keeping track of the entire lineage of data. We also show how our framework can be applied on a simple example of data lake.},
	booktitle = {Proceedings of the 26th international database engineered applications symposium},
	publisher = {Association for Computing Machinery},
	author = {Guyot, Alexis and Gillet, Annabelle and Leclercq, Eric and Cullot, Nadine},
	year = {2022},
	note = {Number of pages: 9
Place: Budapest, Hungary},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Philosophical Paper},
	pages = {75--83},
}

@inproceedings{zhou_radvizbased_2017,
	address = {New York, NY, USA},
	series = {{VINCI} '17},
	title = {A radviz-based visualization for understanding fuzzy clustering results},
	isbn = {978-1-4503-5292-5},
	url = {https://doi.org/10.1145/3105971.3105980},
	doi = {10.1145/3105971.3105980},
	abstract = {Fuzzy clustering analysis is an effective method to describe the uncertainty relationship between data objects and clusters. However, fuzzy clustering results will become complex and high-dimensional membership degree matrixes when they contain a large number of data points and multiple clusters. In this paper, we propose a Radviz-based interactive visualization to help users understand fuzzy clustering results. Firstly, we utilize the projection mechanism of Radviz to map the membership degree matrixes onto planar and radial pictures, in which data points with low membership uncertainty are located near Radviz circumference, while the others are scattered in the center of Radviz circle. To provide an informative interactive visualization, we then improve traditional Radviz visualization in many aspects, including implementing an optimal and uneven placement of dimension anchors by using the Prim algorithm, designing visual codings of data points and dimension arcs to express statistical information, combining chord diagram to depict the sharing relationship between clusters, and offering a set of interactions to support deeper exploration. Finally, we use a case study to illustrate the effectiveness and usefulness of our visualization.},
	booktitle = {Proceedings of the 10th international symposium on visual information communication and interaction},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Fangfang and Chen, Minghui and Wang, Zeyu and Luo, Feng and Luo, Xiaobo and Huang, Wei and Chen, Yi and Zhao, Ying},
	year = {2017},
	note = {Number of pages: 7
Place: Bangkok, Thailand},
	keywords = {RInteractive: Yes, cluster:Visual Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {9--15},
}
